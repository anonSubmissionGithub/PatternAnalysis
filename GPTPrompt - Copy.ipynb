{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7cb2979-a4f9-4e81-a849-17404261f274",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b6747d-d33a-42b2-a301-33ce4d63c473",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "010fc932-3f1f-46cd-8d5b-017fba13c4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36914130-b790-42d0-b515-4d6c0503da5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = 'key'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "435ef736-5559-42f7-9ee0-f3eef82695f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: chardet in c:\\users\\samih\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (5.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install chardet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755d5532-c981-41a7-b3b1-806f5e6fea15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a481c72-b8e7-4623-b333-f6706e7a7b28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-4o is an iteration of the GPT (Generative Pre-trained Transformer) series developed by OpenAI. While specific technical details of GPT-4o may not be explicitly available, I can provide a general understanding based on how GPT models typically work and any improvements that might be expected from a more advanced version:\n",
      "\n",
      "1. **Transformer Architecture**: Like its predecessors, GPT-4o is built on the transformer architecture, which uses mechanisms like self-attention and feed-forward neural networks to process and generate text. This architecture allows the model to understand the context and relationships between words in a sentence.\n",
      "\n",
      "2. **Pre-training and Fine-tuning**:\n",
      "   - **Pre-training**: The model is pre-trained on a large corpus of text from the internet. During this phase, it learns to predict the next word in a sentence, thereby capturing grammar, facts, reasoning abilities, and some level of world knowledge.\n",
      "   - **Fine-tuning**: After pre-training, GPT-4o is fine-tuned on specific datasets with human-curated examples and instructions to ensure it behaves in useful and safe ways. This phase helps it become more adept at understanding instructions and improving its interactive capabilities.\n",
      "\n",
      "3. **Scalability and Parameters**: As an advanced version in the series, GPT-4o likely contains a larger number of parameters than its predecessors (such as GPT-3 or GPT-4). This increased size allows it to capture more intricate patterns in data, potentially improving fluency, coherence, and task-specific performance.\n",
      "\n",
      "4. **Enhanced Alignment and Safety Measures**: Recent models have prioritized ensuring the AI behaves consistently with human values and produces outputs that are safe and ethical. GPT-4o probably incorporates more sophisticated alignment techniques, including reinforcement learning with human feedback (RLHF) to minimize harmful or biased responses.\n",
      "\n",
      "5. **Multimodal Capabilities**: There might also be enhanced or new capabilities for interpreting and generating not just text but also other types of data (images, sounds, etc.), given the trend towards making these models more versatile (e.g., GPT-4 included early forms of multimodal processing).\n",
      "\n",
      "6. **Efficiency Improvements**: Optimizations in training, inference speed, and computational efficiency are also likely, contributing to the model being more cost-effective to deploy and easier to integrate into various applications.\n",
      "\n",
      "It's worth noting that OpenAI typically publishes more detailed explanation and architecture documentation for their major releases, often accompanied by research papers outlining the latest features, enhancements, and design decisions for models like GPT-4o.\n"
     ]
    }
   ],
   "source": [
    "response = openai.chat.completions.create(  # ‚úÖ Use the new API format\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Can you explain how GPT-4o works?\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "assistant_reply = response.choices[0].message.content  # ‚úÖ Updated response handling\n",
    "print(assistant_reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e9f4fe49-c573-4045-87b4-1eae7ac0d2ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìú [1/50] Processing: aircrack-ng_aircrack-ng_da087238963c1239fdabd47dc1b65279605aca70.txt (Length: 1868 characters)\n",
      "‚úÖ Successfully processed aircrack-ng_aircrack-ng_da087238963c1239fdabd47dc1b65279605aca70.txt\n",
      "\n",
      "üìú [2/50] Processing: bdwgc_bdwgc_6a93f8e5bcad22137f41b6c60a1c7384baaec2b3.txt (Length: 2613 characters)\n",
      "‚úÖ Successfully processed bdwgc_bdwgc_6a93f8e5bcad22137f41b6c60a1c7384baaec2b3.txt\n",
      "\n",
      "üìú [3/50] Processing: bubblewrap_bubblewrap_d7fc532c42f0e9bf427923bab85433282b3e5117.txt (Length: 3268 characters)\n",
      "‚ùå Invalid JSON response for bubblewrap_bubblewrap_d7fc532c42f0e9bf427923bab85433282b3e5117.txt: ```{\"answer\": \"Yes\", \"reasoning\": \"The previous fix introduced a mandatory use of setsid(), which could break applications that relied on the older behavior of not creating a new session. By making the call to setsid() optional with the --new-session flag, the future candidate commit addresses a potential compatibility issue for applications that did not require a new terminal session. While the change does not directly fix a security vulnerability, it allows for backward compatibility and still\n",
      "\n",
      "üìú [4/50] Processing: cgminer_cgminer_e1c5050734123973b99d181c45e74b2cbb00272e.txt (Length: 5451 characters)\n",
      "‚úÖ Successfully processed cgminer_cgminer_e1c5050734123973b99d181c45e74b2cbb00272e.txt\n",
      "\n",
      "üìú [5/50] Processing: chromium_chromium_dcd538eb3daf6c52d3ebef0a7afea758f6c657c8.txt (Length: 4327 characters)\n",
      "‚úÖ Successfully processed chromium_chromium_dcd538eb3daf6c52d3ebef0a7afea758f6c657c8.txt\n",
      "\n",
      "üìú [6/50] Processing: cJSON_cJSON_94df772485c92866ca417d92137747b2e3b0a917.txt (Length: 3395 characters)\n",
      "‚úÖ Successfully processed cJSON_cJSON_94df772485c92866ca417d92137747b2e3b0a917.txt\n",
      "\n",
      "üìú [7/50] Processing: collectd_collectd_b589096f907052b3a4da2b9ccc9b0e2e888dfc18.txt (Length: 132412 characters)\n",
      "‚ö†Ô∏è Skipping collectd_collectd_b589096f907052b3a4da2b9ccc9b0e2e888dfc18.txt because it exceeds 100000 characters.\n",
      "\n",
      "üìú [8/50] Processing: cups_cups_49fa4983f25b64ec29d548ffa3b9782426007df3.txt (Length: 28526 characters)\n",
      "‚úÖ Successfully processed cups_cups_49fa4983f25b64ec29d548ffa3b9782426007df3.txt\n",
      "\n",
      "üìú [9/50] Processing: curl_curl_2eb8dcf26cb37f09cffe26909a646e702dbcab66.txt (Length: 4410 characters)\n",
      "‚ùå Invalid JSON response for curl_curl_2eb8dcf26cb37f09cffe26909a646e702dbcab66.txt: ```{\"answer\": \"No\", \"reasoning\": \"The candidate commit addresses numerical IP address host handling in HSTS, specifically centralizing the logic to check if a domain is a numerical IP address by replacing the 'isip' function with 'Curl_host_is_ipnum'. The previous fix focused on solving tailmatching issues to prevent cross-domain cookie leakage (CVE-2013-1944). The change in the candidate commit does not relate to the domain comparisons or tailmatching logic in cookies, but rather optimizes and \n",
      "\n",
      "üìú [10/50] Processing: didiwiki_didiwiki_5e5c796617e1712905dc5462b94bd5e6c08d15ea.txt (Length: 3656 characters)\n",
      "‚úÖ Successfully processed didiwiki_didiwiki_5e5c796617e1712905dc5462b94bd5e6c08d15ea.txt\n",
      "\n",
      "üìú [11/50] Processing: domoticz_domoticz_ee70db46f81afa582c96b887b73bcd2a86feda00.txt (Length: 18647 characters)\n",
      "‚úÖ Successfully processed domoticz_domoticz_ee70db46f81afa582c96b887b73bcd2a86feda00.txt\n",
      "\n",
      "üìú [12/50] Processing: dosfstools_dosfstools_e8eff147e9da1185f9afd5b25948153a3b97cf52.txt (Length: 7654 characters)\n",
      "‚úÖ Successfully processed dosfstools_dosfstools_e8eff147e9da1185f9afd5b25948153a3b97cf52.txt\n",
      "\n",
      "üìú [13/50] Processing: Espruino_Espruino_b6d362f6a1f2de0b3e7604848116efb509196bf4.txt (Length: 22152 characters)\n",
      "‚úÖ Successfully processed Espruino_Espruino_b6d362f6a1f2de0b3e7604848116efb509196bf4.txt\n",
      "\n",
      "üìú [14/50] Processing: ettercap_ettercap_e3abe7d7585ecc420a7cab73313216613aadad5a.txt (Length: 3093 characters)\n",
      "‚úÖ Successfully processed ettercap_ettercap_e3abe7d7585ecc420a7cab73313216613aadad5a.txt\n",
      "\n",
      "üìú [15/50] Processing: ext-http_ext-http_17137d4ab1ce81a2cee0fae842340a344ef3da83.txt (Length: 60936 characters)\n",
      "‚úÖ Successfully processed ext-http_ext-http_17137d4ab1ce81a2cee0fae842340a344ef3da83.txt\n",
      "\n",
      "üìú [16/50] Processing: FFmpeg_FFmpeg_9cb4eb772839c5e1de2855d126bf74ff16d13382.txt (Length: 2536 characters)\n",
      "‚úÖ Successfully processed FFmpeg_FFmpeg_9cb4eb772839c5e1de2855d126bf74ff16d13382.txt\n",
      "\n",
      "üìú [17/50] Processing: file_file_f97486ef5dc3e8735440edc4fc8808c63e1a3ef0.txt (Length: 3101 characters)\n",
      "‚úÖ Successfully processed file_file_f97486ef5dc3e8735440edc4fc8808c63e1a3ef0.txt\n",
      "\n",
      "üìú [18/50] Processing: flatpak_flatpak_52346bf187b5a7f1c0fe9075b328b7ad6abe78f6.txt (Length: 37822 characters)\n",
      "‚úÖ Successfully processed flatpak_flatpak_52346bf187b5a7f1c0fe9075b328b7ad6abe78f6.txt\n",
      "\n",
      "üìú [19/50] Processing: FreeRDP_FreeRDP_2ee663f39dc8dac3d9988e847db19b2d7e3ac8c6.txt (Length: 5517 characters)\n",
      "‚úÖ Successfully processed FreeRDP_FreeRDP_2ee663f39dc8dac3d9988e847db19b2d7e3ac8c6.txt\n",
      "\n",
      "üìú [20/50] Processing: gimp_gimp_c21eff4b031acb04fb4dfce8bd5fdfecc2b6524f.txt (Length: 7833 characters)\n",
      "‚úÖ Successfully processed gimp_gimp_c21eff4b031acb04fb4dfce8bd5fdfecc2b6524f.txt\n",
      "\n",
      "üìú [21/50] Processing: git_git_a124133e1e6ab5c7a9fef6d0e6bcb084e3455b46.txt (Length: 43014 characters)\n",
      "‚úÖ Successfully processed git_git_a124133e1e6ab5c7a9fef6d0e6bcb084e3455b46.txt\n",
      "\n",
      "üìú [22/50] Processing: gpac_gpac_d2371b4b204f0a3c0af51ad4e9b491144dd1225c.txt (Length: 284603 characters)\n",
      "‚ö†Ô∏è Skipping gpac_gpac_d2371b4b204f0a3c0af51ad4e9b491144dd1225c.txt because it exceeds 100000 characters.\n",
      "\n",
      "üìú [23/50] Processing: heimdal_heimdal_1a6a6e462dc2ac6111f9e02c6852ddec4849b887.txt (Length: 3523 characters)\n",
      "‚úÖ Successfully processed heimdal_heimdal_1a6a6e462dc2ac6111f9e02c6852ddec4849b887.txt\n",
      "\n",
      "üìú [24/50] Processing: ImageMagick6_ImageMagick6_cb5ec7d98195aa74d5ed299b38eff2a68122f3fa.txt (Length: 3331 characters)\n",
      "‚úÖ Successfully processed ImageMagick6_ImageMagick6_cb5ec7d98195aa74d5ed299b38eff2a68122f3fa.txt\n",
      "\n",
      "üìú [25/50] Processing: ImageMagick_ImageMagick_c4e63ad30bc42da691f2b5f82a24516dd6b4dc70.txt (Length: 3203 characters)\n",
      "‚úÖ Successfully processed ImageMagick_ImageMagick_c4e63ad30bc42da691f2b5f82a24516dd6b4dc70.txt\n",
      "\n",
      "üìú [26/50] Processing: jasper_jasper_44a524e367597af58d6265ae2014468b334d0309.txt (Length: 7270 characters)\n",
      "‚úÖ Successfully processed jasper_jasper_44a524e367597af58d6265ae2014468b334d0309.txt\n",
      "\n",
      "üìú [27/50] Processing: krb5_krb5_83ed75feba32e46f736fcce0d96a0445f29b96c2.txt (Length: 24844 characters)\n",
      "‚úÖ Successfully processed krb5_krb5_83ed75feba32e46f736fcce0d96a0445f29b96c2.txt\n",
      "\n",
      "üìú [28/50] Processing: leptonica_leptonica_c1079bb8e77cdd426759e466729917ca37a3ed9f.txt (Length: 6718 characters)\n",
      "‚úÖ Successfully processed leptonica_leptonica_c1079bb8e77cdd426759e466729917ca37a3ed9f.txt\n",
      "\n",
      "üìú [29/50] Processing: libarchive_libarchive_d0331e8e5b05b475f20b1f3101fe1ad772d7e7e7.txt (Length: 6936 characters)\n",
      "‚úÖ Successfully processed libarchive_libarchive_d0331e8e5b05b475f20b1f3101fe1ad772d7e7e7.txt\n",
      "\n",
      "üìú [30/50] Processing: libav_libav_cd4663dc80323ba64989d0c103d51ad3ee0e9c2f.txt (Length: 4017 characters)\n",
      "‚úÖ Successfully processed libav_libav_cd4663dc80323ba64989d0c103d51ad3ee0e9c2f.txt\n",
      "\n",
      "üìú [31/50] Processing: libexpat_libexpat_c20b758c332d9a13afbbb276d30db1d183a85d43.txt (Length: 54624 characters)\n",
      "‚úÖ Successfully processed libexpat_libexpat_c20b758c332d9a13afbbb276d30db1d183a85d43.txt\n",
      "\n",
      "üìú [32/50] Processing: libgcrypt_libgcrypt_a4c561aab1014c3630bc88faf6f5246fee16b020.txt (Length: 10487 characters)\n",
      "‚úÖ Successfully processed libgcrypt_libgcrypt_a4c561aab1014c3630bc88faf6f5246fee16b020.txt\n",
      "\n",
      "üìú [33/50] Processing: libgd_libgd_1ccfe21e14c4d18336f9da8515cd17db88c3de61.txt (Length: 1861 characters)\n",
      "‚úÖ Successfully processed libgd_libgd_1ccfe21e14c4d18336f9da8515cd17db88c3de61.txt\n",
      "\n",
      "üìú [34/50] Processing: libgit2_libgit2_58a6fe94cb851f71214dbefac3f9bffee437d6fe.txt (Length: 7322 characters)\n",
      "‚úÖ Successfully processed libgit2_libgit2_58a6fe94cb851f71214dbefac3f9bffee437d6fe.txt\n",
      "\n",
      "üìú [35/50] Processing: libjpeg-turbo_libjpeg-turbo_43e84cff1bb2bd8293066f6ac4eb0df61ddddbc6.txt (Length: 4431 characters)\n",
      "‚ùå Invalid JSON response for libjpeg-turbo_libjpeg-turbo_43e84cff1bb2bd8293066f6ac4eb0df61ddddbc6.txt: ```{\"answer\": \"No\", \"reasoning\": \"The candidate commit is not fixing a vulnerability introduced by the previous fix. The previous fix addressed an issue with potential buffer overflow and division by zero in BMP file handling by ensuring that the product of biWidth and input_components does not exceed a certain limit. The candidate commit, on the other hand, exposes and extends a parameter (max pixels) that was previously only used in fuzz testing, allowing applications to limit memory usage by \n",
      "\n",
      "üìú [36/50] Processing: libmodbus_libmodbus_5ccdf5ef79d742640355d1132fa9e2abc7fbaefc.txt (Length: 3231 characters)\n",
      "‚úÖ Successfully processed libmodbus_libmodbus_5ccdf5ef79d742640355d1132fa9e2abc7fbaefc.txt\n",
      "\n",
      "üìú [37/50] Processing: libmysofa_libmysofa_d39a171e9c6a1c44dbdf43f9db6c3fbd887e38c1.txt (Length: 12134 characters)\n",
      "‚úÖ Successfully processed libmysofa_libmysofa_d39a171e9c6a1c44dbdf43f9db6c3fbd887e38c1.txt\n",
      "\n",
      "üìú [38/50] Processing: libndp_libndp_a4892df306e0532487f1634ba6d4c6d4bb381c7f.txt (Length: 6935 characters)\n",
      "‚ùå Invalid JSON response for libndp_libndp_a4892df306e0532487f1634ba6d4c6d4bb381c7f.txt: ```{\"answer\": \"No\", \"reasoning\": \"The candidate commit does not appear to be fixing a new vulnerability introduced by the previous fix. The previous fix was focused on validating the IPv6 hop limit to ensure packets could not have been forwarded by a router, in compliance with RFC4861, which solved a specific CVE by checking the hop limit value. The future candidate commit, on the other hand, introduces the ability to set a destination address in the IPv6 header for Neighbor Solicitation (NS) an\n",
      "\n",
      "üìú [39/50] Processing: libplist_libplist_fbd8494d5e4e46bf2e90cb6116903e404374fb56.txt (Length: 2314 characters)\n",
      "‚úÖ Successfully processed libplist_libplist_fbd8494d5e4e46bf2e90cb6116903e404374fb56.txt\n",
      "\n",
      "üìú [40/50] Processing: LibRaw-demosaic-pack-GPL2_LibRaw-demosaic-pack-GPL2_194f592e205990ea8fce72b6c571c14350aca716.txt (Length: 2935 characters)\n",
      "‚úÖ Successfully processed LibRaw-demosaic-pack-GPL2_LibRaw-demosaic-pack-GPL2_194f592e205990ea8fce72b6c571c14350aca716.txt\n",
      "\n",
      "üìú [41/50] Processing: libreswan_libreswan_2899351224fe2940aec37d7656e1e392c0fe07f0.txt (Length: 45841 characters)\n",
      "‚úÖ Successfully processed libreswan_libreswan_2899351224fe2940aec37d7656e1e392c0fe07f0.txt\n",
      "\n",
      "üìú [42/50] Processing: libsndfile_libsndfile_60b234301adf258786d8b90be5c1d437fc8799e0.txt (Length: 4862 characters)\n",
      "‚úÖ Successfully processed libsndfile_libsndfile_60b234301adf258786d8b90be5c1d437fc8799e0.txt\n",
      "\n",
      "üìú [43/50] Processing: libtiff_libtiff_391e77fcd217e78b2c51342ac3ddb7100ecacdd2.txt (Length: 2615 characters)\n",
      "‚úÖ Successfully processed libtiff_libtiff_391e77fcd217e78b2c51342ac3ddb7100ecacdd2.txt\n",
      "\n",
      "üìú [44/50] Processing: libu2f-host_libu2f-host_e4bb58cc8b6202a421e65f8230217d8ae6e16eb5.txt (Length: 2809 characters)\n",
      "‚úÖ Successfully processed libu2f-host_libu2f-host_e4bb58cc8b6202a421e65f8230217d8ae6e16eb5.txt\n",
      "\n",
      "üìú [45/50] Processing: libuv_libuv_66ab38918c911bcff025562cf06237d7fedaba0c.txt (Length: 19675 characters)\n",
      "‚úÖ Successfully processed libuv_libuv_66ab38918c911bcff025562cf06237d7fedaba0c.txt\n",
      "\n",
      "üìú [46/50] Processing: Little-CMS_Little-CMS_768f70ca405cd3159d990e962d54456773bb8cf8.txt (Length: 3216 characters)\n",
      "‚úÖ Successfully processed Little-CMS_Little-CMS_768f70ca405cd3159d990e962d54456773bb8cf8.txt\n",
      "\n",
      "üìú [47/50] Processing: OpenSC_OpenSC_412a6142c27a5973c61ba540e33cdc22d5608e68.txt (Length: 2404 characters)\n",
      "‚úÖ Successfully processed OpenSC_OpenSC_412a6142c27a5973c61ba540e33cdc22d5608e68.txt\n",
      "\n",
      "üìú [48/50] Processing: PDFGen_PDFGen_ee58aff6918b8bbc3be29b9e3089485ea46ff956.txt (Length: 2241 characters)\n",
      "‚úÖ Successfully processed PDFGen_PDFGen_ee58aff6918b8bbc3be29b9e3089485ea46ff956.txt\n",
      "\n",
      "üìú [49/50] Processing: VeraCrypt_VeraCrypt_f30f9339c9a0b9bbcc6f5ad38804af39db1f479e.txt (Length: 8263 characters)\n",
      "‚ùå Invalid JSON response for VeraCrypt_VeraCrypt_f30f9339c9a0b9bbcc6f5ad38804af39db1f479e.txt: ```{\"answer\": \"No\", \"reasoning\": \"The candidate commit addresses a separate issue related to RAM encryption logic involving the initialization of security parameters and the conditional enabling of RAM encryption based on a defined flag. In the previous fix, the focus was on resolving a vulnerability that allowed unauthorized reading of kernel stack memory, which was addressed by adding validation for the DevicePath length and format, memory allocation checks, and ensuring successful reading of \n",
      "\n",
      "üìú [50/50] Processing: WavPack_WavPack_8e3fe45a7bac31d9a3b558ae0079e2d92a04799e.txt (Length: 4543 characters)\n",
      "‚úÖ Successfully processed WavPack_WavPack_8e3fe45a7bac31d9a3b558ae0079e2d92a04799e.txt\n",
      "\n",
      "‚ö†Ô∏è Errors saved separately in: M:\\FULL_DATA_COLLECTED\\prompts\\gpt-4o\\gpt-4o_errors.xlsx\n",
      "\n",
      "‚úÖ All responses saved successfully in: M:\\FULL_DATA_COLLECTED\\prompts\\gpt-4o\\gpt-4o_results.xlsx\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import openai\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "import chardet  # Detects file encoding\n",
    "\n",
    "# Define paths\n",
    "PROMPT_DIR = r\"M:\\FULL_DATA_COLLECTED\\prompts\"\n",
    "LLM_NAME = \"gpt-4o\"  # Change this if using a different LLM\n",
    "RESULTS_DIR = os.path.join(PROMPT_DIR, LLM_NAME)\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)  # Ensure directory exists\n",
    "\n",
    "# Prepare storage for results\n",
    "results = []\n",
    "error_logs = []\n",
    "\n",
    "# List all prompt files\n",
    "prompt_files = [f for f in os.listdir(PROMPT_DIR) if f.endswith(\".txt\")]\n",
    "\n",
    "for idx, prompt_file in enumerate(prompt_files, start=1):\n",
    "    prompt_path = os.path.join(PROMPT_DIR, prompt_file)\n",
    "\n",
    "    # Detect file encoding\n",
    "    with open(prompt_path, \"rb\") as f:\n",
    "        raw_data = f.read()\n",
    "        encoding = chardet.detect(raw_data)['encoding']\n",
    "    \n",
    "    # Read the prompt content with detected encoding\n",
    "    try:\n",
    "        with open(prompt_path, \"r\", encoding=encoding) as f:\n",
    "            prompt_content = f.read().strip()\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Encoding issue with {prompt_file}: {e}\")\n",
    "        error_logs.append([prompt_file, \"Encoding Error\", str(e)])\n",
    "        continue  # Skip this file and move to next\n",
    "\n",
    "    # Print prompt details\n",
    "    prompt_length = len(prompt_content)\n",
    "    print(f\"\\nüìú [{idx}/{len(prompt_files)}] Processing: {prompt_file} (Length: {prompt_length} characters)\")\n",
    "\n",
    "    # **Skip processing if the prompt is too long**\n",
    "    MAX_LENGTH = 100000  # Adjust this limit as needed\n",
    "    if prompt_length > MAX_LENGTH:\n",
    "        print(f\"‚ö†Ô∏è Skipping {prompt_file} because it exceeds {MAX_LENGTH} characters.\")\n",
    "        error_logs.append([prompt_file, \"Skipped\", \"Prompt too long\"])\n",
    "        continue  # Skip to next prompt\n",
    "\n",
    "    # Send the prompt to GPT-4o\n",
    "    try:\n",
    "        response = openai.chat.completions.create(\n",
    "            model=LLM_NAME,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a cybersecurity expert analyzing software vulnerabilities. \"\n",
    "                                              \"Provide your response **only in the following strict JSON format**:\\n\"\n",
    "                                              \"{\\n\"\n",
    "                                              '    \"answer\": \"Yes\" or \"No\",\\n'\n",
    "                                              '    \"reasoning\": \"Concise but detailed explanation.\"\\n'\n",
    "                                              \"}\\n\"\n",
    "                                              \"Do not include any extra text outside this JSON structure.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt_content}\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Extract response\n",
    "        assistant_reply = response.choices[0].message.content.strip()\n",
    "\n",
    "        # ‚úÖ **Remove surrounding triple backticks (` ```json ... ``` `)**\n",
    "        if assistant_reply.startswith(\"```json\"):\n",
    "            assistant_reply = assistant_reply[7:]  # Remove starting ```json\n",
    "        if assistant_reply.endswith(\"```\"):\n",
    "            assistant_reply = assistant_reply[:-3]  # Remove ending ```\n",
    "\n",
    "        # Parse JSON response\n",
    "        try:\n",
    "            response_json = json.loads(assistant_reply)  # Ensure it's valid JSON\n",
    "            answer = response_json.get(\"answer\", \"N/A\")\n",
    "            reasoning = response_json.get(\"reasoning\", \"N/A\")\n",
    "\n",
    "            # Store results\n",
    "            results.append([prompt_file, answer, reasoning])\n",
    "            print(f\"‚úÖ Successfully processed {prompt_file}\")\n",
    "\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"‚ùå Invalid JSON response for {prompt_file}: {assistant_reply[:500]}\")\n",
    "            error_logs.append([prompt_file, \"Invalid JSON\", assistant_reply])\n",
    "            results.append([prompt_file, \"Error\", \"Invalid JSON response\"])\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error processing {prompt_file}: {e}\")\n",
    "        error_logs.append([prompt_file, \"API Error\", str(e)])\n",
    "        results.append([prompt_file, \"Error\", str(e)])\n",
    "\n",
    "    # **Longer delay to prevent API rate limits**\n",
    "    time.sleep(10)  # Adjust if needed\n",
    "\n",
    "# Convert results to DataFrame\n",
    "df_results = pd.DataFrame(results, columns=[\"Prompt File\", \"Answer\", \"Reasoning\"])\n",
    "\n",
    "# Save to Excel\n",
    "xlsx_path = os.path.join(RESULTS_DIR, f\"{LLM_NAME}_results.xlsx\")\n",
    "df_results.to_excel(xlsx_path, index=False)\n",
    "\n",
    "# Save error logs separately\n",
    "if error_logs:\n",
    "    df_errors = pd.DataFrame(error_logs, columns=[\"Prompt File\", \"Error Type\", \"Details\"])\n",
    "    error_xlsx_path = os.path.join(RESULTS_DIR, f\"{LLM_NAME}_errors.xlsx\")\n",
    "    df_errors.to_excel(error_xlsx_path, index=False)\n",
    "    print(f\"\\n‚ö†Ô∏è Errors saved separately in: {error_xlsx_path}\")\n",
    "\n",
    "print(f\"\\n‚úÖ All responses saved successfully in: {xlsx_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a2f53c6f-883d-4627-8ed4-0a68b84adb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modified to handle errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cf339ae3-973d-4009-bb63-4af1ba0f2b0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìú [1/50] Processing: aircrack-ng_aircrack-ng_da087238963c1239fdabd47dc1b65279605aca70.txt (Length: 1868 characters)\n",
      "‚úÖ Successfully processed aircrack-ng_aircrack-ng_da087238963c1239fdabd47dc1b65279605aca70.txt\n",
      "\n",
      "üìú [2/50] Processing: bdwgc_bdwgc_6a93f8e5bcad22137f41b6c60a1c7384baaec2b3.txt (Length: 2613 characters)\n",
      "‚úÖ Successfully processed bdwgc_bdwgc_6a93f8e5bcad22137f41b6c60a1c7384baaec2b3.txt\n",
      "\n",
      "üìú [3/50] Processing: bubblewrap_bubblewrap_d7fc532c42f0e9bf427923bab85433282b3e5117.txt (Length: 3268 characters)\n",
      "‚úÖ Successfully processed bubblewrap_bubblewrap_d7fc532c42f0e9bf427923bab85433282b3e5117.txt\n",
      "\n",
      "üìú [4/50] Processing: cgminer_cgminer_e1c5050734123973b99d181c45e74b2cbb00272e.txt (Length: 5451 characters)\n",
      "‚úÖ Successfully processed cgminer_cgminer_e1c5050734123973b99d181c45e74b2cbb00272e.txt\n",
      "\n",
      "üìú [5/50] Processing: chromium_chromium_dcd538eb3daf6c52d3ebef0a7afea758f6c657c8.txt (Length: 4327 characters)\n",
      "‚úÖ Successfully processed chromium_chromium_dcd538eb3daf6c52d3ebef0a7afea758f6c657c8.txt\n",
      "\n",
      "üìú [6/50] Processing: cJSON_cJSON_94df772485c92866ca417d92137747b2e3b0a917.txt (Length: 3395 characters)\n",
      "‚úÖ Successfully processed cJSON_cJSON_94df772485c92866ca417d92137747b2e3b0a917.txt\n",
      "\n",
      "üìú [7/50] Processing: collectd_collectd_b589096f907052b3a4da2b9ccc9b0e2e888dfc18.txt (Length: 132412 characters)\n",
      "‚ö†Ô∏è Skipping collectd_collectd_b589096f907052b3a4da2b9ccc9b0e2e888dfc18.txt because it exceeds 100000 characters.\n",
      "\n",
      "üìú [8/50] Processing: cups_cups_49fa4983f25b64ec29d548ffa3b9782426007df3.txt (Length: 28526 characters)\n",
      "‚úÖ Successfully processed cups_cups_49fa4983f25b64ec29d548ffa3b9782426007df3.txt\n",
      "\n",
      "üìú [9/50] Processing: curl_curl_2eb8dcf26cb37f09cffe26909a646e702dbcab66.txt (Length: 4410 characters)\n",
      "‚ö†Ô∏è Attempt 1/3: Invalid JSON response for curl_curl_2eb8dcf26cb37f09cffe26909a646e702dbcab66.txt, retrying...\n",
      "‚úÖ Successfully processed curl_curl_2eb8dcf26cb37f09cffe26909a646e702dbcab66.txt\n",
      "\n",
      "üìú [10/50] Processing: didiwiki_didiwiki_5e5c796617e1712905dc5462b94bd5e6c08d15ea.txt (Length: 3656 characters)\n",
      "‚úÖ Successfully processed didiwiki_didiwiki_5e5c796617e1712905dc5462b94bd5e6c08d15ea.txt\n",
      "\n",
      "üìú [11/50] Processing: domoticz_domoticz_ee70db46f81afa582c96b887b73bcd2a86feda00.txt (Length: 18647 characters)\n",
      "‚úÖ Successfully processed domoticz_domoticz_ee70db46f81afa582c96b887b73bcd2a86feda00.txt\n",
      "\n",
      "üìú [12/50] Processing: dosfstools_dosfstools_e8eff147e9da1185f9afd5b25948153a3b97cf52.txt (Length: 7654 characters)\n",
      "‚úÖ Successfully processed dosfstools_dosfstools_e8eff147e9da1185f9afd5b25948153a3b97cf52.txt\n",
      "\n",
      "üìú [13/50] Processing: Espruino_Espruino_b6d362f6a1f2de0b3e7604848116efb509196bf4.txt (Length: 22152 characters)\n",
      "‚úÖ Successfully processed Espruino_Espruino_b6d362f6a1f2de0b3e7604848116efb509196bf4.txt\n",
      "\n",
      "üìú [14/50] Processing: ettercap_ettercap_e3abe7d7585ecc420a7cab73313216613aadad5a.txt (Length: 3093 characters)\n",
      "‚úÖ Successfully processed ettercap_ettercap_e3abe7d7585ecc420a7cab73313216613aadad5a.txt\n",
      "\n",
      "üìú [15/50] Processing: ext-http_ext-http_17137d4ab1ce81a2cee0fae842340a344ef3da83.txt (Length: 60936 characters)\n",
      "‚úÖ Successfully processed ext-http_ext-http_17137d4ab1ce81a2cee0fae842340a344ef3da83.txt\n",
      "\n",
      "üìú [16/50] Processing: FFmpeg_FFmpeg_9cb4eb772839c5e1de2855d126bf74ff16d13382.txt (Length: 2536 characters)\n",
      "‚ö†Ô∏è Attempt 1/3: Invalid JSON response for FFmpeg_FFmpeg_9cb4eb772839c5e1de2855d126bf74ff16d13382.txt, retrying...\n",
      "‚úÖ Successfully processed FFmpeg_FFmpeg_9cb4eb772839c5e1de2855d126bf74ff16d13382.txt\n",
      "\n",
      "üìú [17/50] Processing: file_file_f97486ef5dc3e8735440edc4fc8808c63e1a3ef0.txt (Length: 3101 characters)\n",
      "‚úÖ Successfully processed file_file_f97486ef5dc3e8735440edc4fc8808c63e1a3ef0.txt\n",
      "\n",
      "üìú [18/50] Processing: flatpak_flatpak_52346bf187b5a7f1c0fe9075b328b7ad6abe78f6.txt (Length: 37822 characters)\n",
      "‚úÖ Successfully processed flatpak_flatpak_52346bf187b5a7f1c0fe9075b328b7ad6abe78f6.txt\n",
      "\n",
      "üìú [19/50] Processing: FreeRDP_FreeRDP_2ee663f39dc8dac3d9988e847db19b2d7e3ac8c6.txt (Length: 5517 characters)\n",
      "‚ö†Ô∏è Attempt 1/3: Invalid JSON response for FreeRDP_FreeRDP_2ee663f39dc8dac3d9988e847db19b2d7e3ac8c6.txt, retrying...\n",
      "‚úÖ Successfully processed FreeRDP_FreeRDP_2ee663f39dc8dac3d9988e847db19b2d7e3ac8c6.txt\n",
      "\n",
      "üìú [20/50] Processing: gimp_gimp_c21eff4b031acb04fb4dfce8bd5fdfecc2b6524f.txt (Length: 7833 characters)\n",
      "‚úÖ Successfully processed gimp_gimp_c21eff4b031acb04fb4dfce8bd5fdfecc2b6524f.txt\n",
      "\n",
      "üìú [21/50] Processing: git_git_a124133e1e6ab5c7a9fef6d0e6bcb084e3455b46.txt (Length: 43014 characters)\n",
      "‚úÖ Successfully processed git_git_a124133e1e6ab5c7a9fef6d0e6bcb084e3455b46.txt\n",
      "\n",
      "üìú [22/50] Processing: gpac_gpac_d2371b4b204f0a3c0af51ad4e9b491144dd1225c.txt (Length: 284603 characters)\n",
      "‚ö†Ô∏è Skipping gpac_gpac_d2371b4b204f0a3c0af51ad4e9b491144dd1225c.txt because it exceeds 100000 characters.\n",
      "\n",
      "üìú [23/50] Processing: heimdal_heimdal_1a6a6e462dc2ac6111f9e02c6852ddec4849b887.txt (Length: 3523 characters)\n",
      "‚úÖ Successfully processed heimdal_heimdal_1a6a6e462dc2ac6111f9e02c6852ddec4849b887.txt\n",
      "\n",
      "üìú [24/50] Processing: ImageMagick6_ImageMagick6_cb5ec7d98195aa74d5ed299b38eff2a68122f3fa.txt (Length: 3331 characters)\n",
      "‚úÖ Successfully processed ImageMagick6_ImageMagick6_cb5ec7d98195aa74d5ed299b38eff2a68122f3fa.txt\n",
      "\n",
      "üìú [25/50] Processing: ImageMagick_ImageMagick_c4e63ad30bc42da691f2b5f82a24516dd6b4dc70.txt (Length: 3203 characters)\n",
      "‚úÖ Successfully processed ImageMagick_ImageMagick_c4e63ad30bc42da691f2b5f82a24516dd6b4dc70.txt\n",
      "\n",
      "üìú [26/50] Processing: jasper_jasper_44a524e367597af58d6265ae2014468b334d0309.txt (Length: 7270 characters)\n",
      "‚úÖ Successfully processed jasper_jasper_44a524e367597af58d6265ae2014468b334d0309.txt\n",
      "\n",
      "üìú [27/50] Processing: krb5_krb5_83ed75feba32e46f736fcce0d96a0445f29b96c2.txt (Length: 24844 characters)\n",
      "‚ö†Ô∏è Attempt 1/3: Invalid JSON response for krb5_krb5_83ed75feba32e46f736fcce0d96a0445f29b96c2.txt, retrying...\n",
      "‚úÖ Successfully processed krb5_krb5_83ed75feba32e46f736fcce0d96a0445f29b96c2.txt\n",
      "\n",
      "üìú [28/50] Processing: leptonica_leptonica_c1079bb8e77cdd426759e466729917ca37a3ed9f.txt (Length: 6718 characters)\n",
      "‚úÖ Successfully processed leptonica_leptonica_c1079bb8e77cdd426759e466729917ca37a3ed9f.txt\n",
      "\n",
      "üìú [29/50] Processing: libarchive_libarchive_d0331e8e5b05b475f20b1f3101fe1ad772d7e7e7.txt (Length: 6936 characters)\n",
      "‚úÖ Successfully processed libarchive_libarchive_d0331e8e5b05b475f20b1f3101fe1ad772d7e7e7.txt\n",
      "\n",
      "üìú [30/50] Processing: libav_libav_cd4663dc80323ba64989d0c103d51ad3ee0e9c2f.txt (Length: 4017 characters)\n",
      "‚úÖ Successfully processed libav_libav_cd4663dc80323ba64989d0c103d51ad3ee0e9c2f.txt\n",
      "\n",
      "üìú [31/50] Processing: libexpat_libexpat_c20b758c332d9a13afbbb276d30db1d183a85d43.txt (Length: 54624 characters)\n",
      "‚úÖ Successfully processed libexpat_libexpat_c20b758c332d9a13afbbb276d30db1d183a85d43.txt\n",
      "\n",
      "üìú [32/50] Processing: libgcrypt_libgcrypt_a4c561aab1014c3630bc88faf6f5246fee16b020.txt (Length: 10487 characters)\n",
      "‚úÖ Successfully processed libgcrypt_libgcrypt_a4c561aab1014c3630bc88faf6f5246fee16b020.txt\n",
      "\n",
      "üìú [33/50] Processing: libgd_libgd_1ccfe21e14c4d18336f9da8515cd17db88c3de61.txt (Length: 1861 characters)\n",
      "‚úÖ Successfully processed libgd_libgd_1ccfe21e14c4d18336f9da8515cd17db88c3de61.txt\n",
      "\n",
      "üìú [34/50] Processing: libgit2_libgit2_58a6fe94cb851f71214dbefac3f9bffee437d6fe.txt (Length: 7322 characters)\n",
      "‚úÖ Successfully processed libgit2_libgit2_58a6fe94cb851f71214dbefac3f9bffee437d6fe.txt\n",
      "\n",
      "üìú [35/50] Processing: libjpeg-turbo_libjpeg-turbo_43e84cff1bb2bd8293066f6ac4eb0df61ddddbc6.txt (Length: 4431 characters)\n",
      "‚úÖ Successfully processed libjpeg-turbo_libjpeg-turbo_43e84cff1bb2bd8293066f6ac4eb0df61ddddbc6.txt\n",
      "\n",
      "üìú [36/50] Processing: libmodbus_libmodbus_5ccdf5ef79d742640355d1132fa9e2abc7fbaefc.txt (Length: 3231 characters)\n",
      "‚úÖ Successfully processed libmodbus_libmodbus_5ccdf5ef79d742640355d1132fa9e2abc7fbaefc.txt\n",
      "\n",
      "üìú [37/50] Processing: libmysofa_libmysofa_d39a171e9c6a1c44dbdf43f9db6c3fbd887e38c1.txt (Length: 12134 characters)\n",
      "‚úÖ Successfully processed libmysofa_libmysofa_d39a171e9c6a1c44dbdf43f9db6c3fbd887e38c1.txt\n",
      "\n",
      "üìú [38/50] Processing: libndp_libndp_a4892df306e0532487f1634ba6d4c6d4bb381c7f.txt (Length: 6935 characters)\n",
      "‚úÖ Successfully processed libndp_libndp_a4892df306e0532487f1634ba6d4c6d4bb381c7f.txt\n",
      "\n",
      "üìú [39/50] Processing: libplist_libplist_fbd8494d5e4e46bf2e90cb6116903e404374fb56.txt (Length: 2314 characters)\n",
      "‚úÖ Successfully processed libplist_libplist_fbd8494d5e4e46bf2e90cb6116903e404374fb56.txt\n",
      "\n",
      "üìú [40/50] Processing: LibRaw-demosaic-pack-GPL2_LibRaw-demosaic-pack-GPL2_194f592e205990ea8fce72b6c571c14350aca716.txt (Length: 2935 characters)\n",
      "‚ö†Ô∏è Attempt 1/3: Invalid JSON response for LibRaw-demosaic-pack-GPL2_LibRaw-demosaic-pack-GPL2_194f592e205990ea8fce72b6c571c14350aca716.txt, retrying...\n",
      "‚úÖ Successfully processed LibRaw-demosaic-pack-GPL2_LibRaw-demosaic-pack-GPL2_194f592e205990ea8fce72b6c571c14350aca716.txt\n",
      "\n",
      "üìú [41/50] Processing: libreswan_libreswan_2899351224fe2940aec37d7656e1e392c0fe07f0.txt (Length: 45841 characters)\n",
      "‚úÖ Successfully processed libreswan_libreswan_2899351224fe2940aec37d7656e1e392c0fe07f0.txt\n",
      "\n",
      "üìú [42/50] Processing: libsndfile_libsndfile_60b234301adf258786d8b90be5c1d437fc8799e0.txt (Length: 4862 characters)\n",
      "‚úÖ Successfully processed libsndfile_libsndfile_60b234301adf258786d8b90be5c1d437fc8799e0.txt\n",
      "\n",
      "üìú [43/50] Processing: libtiff_libtiff_391e77fcd217e78b2c51342ac3ddb7100ecacdd2.txt (Length: 2615 characters)\n",
      "‚úÖ Successfully processed libtiff_libtiff_391e77fcd217e78b2c51342ac3ddb7100ecacdd2.txt\n",
      "\n",
      "üìú [44/50] Processing: libu2f-host_libu2f-host_e4bb58cc8b6202a421e65f8230217d8ae6e16eb5.txt (Length: 2809 characters)\n",
      "‚úÖ Successfully processed libu2f-host_libu2f-host_e4bb58cc8b6202a421e65f8230217d8ae6e16eb5.txt\n",
      "\n",
      "üìú [45/50] Processing: libuv_libuv_66ab38918c911bcff025562cf06237d7fedaba0c.txt (Length: 19675 characters)\n",
      "‚úÖ Successfully processed libuv_libuv_66ab38918c911bcff025562cf06237d7fedaba0c.txt\n",
      "\n",
      "üìú [46/50] Processing: Little-CMS_Little-CMS_768f70ca405cd3159d990e962d54456773bb8cf8.txt (Length: 3216 characters)\n",
      "‚úÖ Successfully processed Little-CMS_Little-CMS_768f70ca405cd3159d990e962d54456773bb8cf8.txt\n",
      "\n",
      "üìú [47/50] Processing: OpenSC_OpenSC_412a6142c27a5973c61ba540e33cdc22d5608e68.txt (Length: 2404 characters)\n",
      "‚úÖ Successfully processed OpenSC_OpenSC_412a6142c27a5973c61ba540e33cdc22d5608e68.txt\n",
      "\n",
      "üìú [48/50] Processing: PDFGen_PDFGen_ee58aff6918b8bbc3be29b9e3089485ea46ff956.txt (Length: 2241 characters)\n",
      "‚úÖ Successfully processed PDFGen_PDFGen_ee58aff6918b8bbc3be29b9e3089485ea46ff956.txt\n",
      "\n",
      "üìú [49/50] Processing: VeraCrypt_VeraCrypt_f30f9339c9a0b9bbcc6f5ad38804af39db1f479e.txt (Length: 8263 characters)\n",
      "‚úÖ Successfully processed VeraCrypt_VeraCrypt_f30f9339c9a0b9bbcc6f5ad38804af39db1f479e.txt\n",
      "\n",
      "üìú [50/50] Processing: WavPack_WavPack_8e3fe45a7bac31d9a3b558ae0079e2d92a04799e.txt (Length: 4543 characters)\n",
      "‚úÖ Successfully processed WavPack_WavPack_8e3fe45a7bac31d9a3b558ae0079e2d92a04799e.txt\n",
      "\n",
      "‚úÖ All responses saved successfully in: M:\\FULL_DATA_COLLECTED\\prompts\\gpt-4o\\gpt-4o_results.xlsx\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import openai\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "import chardet  # Detects file encoding\n",
    "\n",
    "# Define paths\n",
    "PROMPT_DIR = r\"M:\\FULL_DATA_COLLECTED\\prompts\"\n",
    "LLM_NAME = \"gpt-4o\"  # Change this if using a different LLM\n",
    "RESULTS_DIR = os.path.join(PROMPT_DIR, LLM_NAME)\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)  # Ensure directory exists\n",
    "\n",
    "# Prepare storage for results\n",
    "results = []\n",
    "error_logs = []\n",
    "\n",
    "# List all prompt files\n",
    "prompt_files = [f for f in os.listdir(PROMPT_DIR) if f.endswith(\".txt\")]\n",
    "\n",
    "for idx, prompt_file in enumerate(prompt_files, start=1):\n",
    "    prompt_path = os.path.join(PROMPT_DIR, prompt_file)\n",
    "\n",
    "    # Detect file encoding\n",
    "    with open(prompt_path, \"rb\") as f:\n",
    "        raw_data = f.read()\n",
    "        encoding = chardet.detect(raw_data)['encoding']\n",
    "    \n",
    "    # Read the prompt content with detected encoding\n",
    "    try:\n",
    "        with open(prompt_path, \"r\", encoding=encoding) as f:\n",
    "            prompt_content = f.read().strip()\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Encoding issue with {prompt_file}: {e}\")\n",
    "        error_logs.append([prompt_file, \"Encoding Error\", str(e)])\n",
    "        continue  # Skip this file and move to next\n",
    "\n",
    "    # Print prompt details\n",
    "    prompt_length = len(prompt_content)\n",
    "    print(f\"\\nüìú [{idx}/{len(prompt_files)}] Processing: {prompt_file} (Length: {prompt_length} characters)\")\n",
    "\n",
    "    # **Skip processing if the prompt is too long**\n",
    "    MAX_LENGTH = 100000  # Adjust this limit as needed\n",
    "    if prompt_length > MAX_LENGTH:\n",
    "        print(f\"‚ö†Ô∏è Skipping {prompt_file} because it exceeds {MAX_LENGTH} characters.\")\n",
    "        results.append([prompt_file, \"Skipped\", \"Prompt too long\"])\n",
    "        continue  # Skip to next prompt\n",
    "\n",
    "    # **Retry logic for invalid JSON responses**\n",
    "    retry_attempts = 3\n",
    "    for attempt in range(retry_attempts):\n",
    "        try:\n",
    "            response = openai.chat.completions.create(\n",
    "                model=LLM_NAME,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a cybersecurity expert analyzing software vulnerabilities. \"\n",
    "                                                  \"Provide your response **only in the following strict JSON format**:\\n\"\n",
    "                                                  \"{\\n\"\n",
    "                                                  '    \"answer\": \"Yes\" or \"No\",\\n'\n",
    "                                                  '    \"reasoning\": \"Concise but detailed explanation.\"\\n'\n",
    "                                                  \"}\\n\"\n",
    "                                                  \"Do not include any extra text outside this JSON structure.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt_content}\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            # Extract response\n",
    "            assistant_reply = response.choices[0].message.content.strip()\n",
    "\n",
    "            # ‚úÖ **Attempt to clean JSON formatting**\n",
    "            if assistant_reply.startswith(\"```json\"):\n",
    "                assistant_reply = assistant_reply[7:]  # Remove starting ```json\n",
    "            if assistant_reply.endswith(\"```\"):\n",
    "                assistant_reply = assistant_reply[:-3]  # Remove ending ```\n",
    "\n",
    "            try:\n",
    "                response_json = json.loads(assistant_reply)  # Ensure it's valid JSON\n",
    "                answer = response_json.get(\"answer\", \"N/A\")\n",
    "                reasoning = response_json.get(\"reasoning\", \"N/A\")\n",
    "\n",
    "                # Store results\n",
    "                results.append([prompt_file, answer, reasoning])\n",
    "                print(f\"‚úÖ Successfully processed {prompt_file}\")\n",
    "                break  # **Exit retry loop on success**\n",
    "\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"‚ö†Ô∏è Attempt {attempt + 1}/{retry_attempts}: Invalid JSON response for {prompt_file}, retrying...\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error processing {prompt_file}: {e}\")\n",
    "            error_logs.append([prompt_file, \"API Error\", str(e)])\n",
    "            results.append([prompt_file, \"Error\", str(e)])\n",
    "            break  # Stop retrying for API errors\n",
    "\n",
    "        # **Delay before retrying**\n",
    "        time.sleep(10)\n",
    "\n",
    "    else:\n",
    "        # If all retry attempts fail, log as invalid JSON\n",
    "        print(f\"‚ùå All {retry_attempts} attempts failed for {prompt_file}. Marking as error.\")\n",
    "        error_logs.append([prompt_file, \"Invalid JSON\", assistant_reply])\n",
    "        results.append([prompt_file, \"Error\", \"Invalid JSON response\"])\n",
    "\n",
    "    # **Longer delay to prevent API rate limits**\n",
    "    time.sleep(10)  # Adjust if needed\n",
    "\n",
    "# Convert results to DataFrame\n",
    "df_results = pd.DataFrame(results, columns=[\"Prompt File\", \"Answer\", \"Reasoning\"])\n",
    "\n",
    "# Save to Excel\n",
    "xlsx_path = os.path.join(RESULTS_DIR, f\"{LLM_NAME}_results.xlsx\")\n",
    "df_results.to_excel(xlsx_path, index=False)\n",
    "\n",
    "# Save error logs separately\n",
    "if error_logs:\n",
    "    df_errors = pd.DataFrame(error_logs, columns=[\"Prompt File\", \"Error Type\", \"Details\"])\n",
    "    error_xlsx_path = os.path.join(RESULTS_DIR, f\"{LLM_NAME}_errors.xlsx\")\n",
    "    df_errors.to_excel(error_xlsx_path, index=False)\n",
    "    print(f\"\\n‚ö†Ô∏è Errors saved separately in: {error_xlsx_path}\")\n",
    "\n",
    "print(f\"\\n‚úÖ All responses saved successfully in: {xlsx_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8a8a67-da4b-491a-a901-4d9f2e97f606",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompts2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a0749eec-7334-4d45-94df-e1964773138f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìú [1/50] Processing: aircrack-ng_aircrack-ng_da087238963c1239fdabd47dc1b65279605aca70.txt (Length: 2146 characters)\n",
      "‚ùå Error processing aircrack-ng_aircrack-ng_da087238963c1239fdabd47dc1b65279605aca70.txt: module 'openai' has no attribute 'chat'\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 105\u001b[0m\n\u001b[0;32m    102\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend([prompt_file, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid JSON response\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    104\u001b[0m     \u001b[38;5;66;03m# **Longer delay to prevent API rate limits**\u001b[39;00m\n\u001b[1;32m--> 105\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Adjust if needed\u001b[39;00m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;66;03m# Convert results to DataFrame\u001b[39;00m\n\u001b[0;32m    108\u001b[0m df_results \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(results, columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrompt File\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReasoning\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import openai\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "import chardet  # Detects file encoding\n",
    "\n",
    "# Define paths\n",
    "PROMPT_DIR = r\"M:\\FULL_DATA_COLLECTED\\prompts2\"\n",
    "LLM_NAME = \"gpt-4o\"  # Change this if using a different LLM\n",
    "RESULTS_DIR = os.path.join(PROMPT_DIR, LLM_NAME)\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)  # Ensure directory exists\n",
    "\n",
    "# Prepare storage for results\n",
    "results = []\n",
    "error_logs = []\n",
    "\n",
    "# List all prompt files\n",
    "prompt_files = [f for f in os.listdir(PROMPT_DIR) if f.endswith(\".txt\")]\n",
    "\n",
    "for idx, prompt_file in enumerate(prompt_files, start=1):\n",
    "    prompt_path = os.path.join(PROMPT_DIR, prompt_file)\n",
    "\n",
    "    # Detect file encoding\n",
    "    with open(prompt_path, \"rb\") as f:\n",
    "        raw_data = f.read()\n",
    "        encoding = chardet.detect(raw_data)['encoding']\n",
    "    \n",
    "    # Read the prompt content with detected encoding\n",
    "    try:\n",
    "        with open(prompt_path, \"r\", encoding=encoding) as f:\n",
    "            prompt_content = f.read().strip()\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Encoding issue with {prompt_file}: {e}\")\n",
    "        error_logs.append([prompt_file, \"Encoding Error\", str(e)])\n",
    "        continue  # Skip this file and move to next\n",
    "\n",
    "    # Print prompt details\n",
    "    prompt_length = len(prompt_content)\n",
    "    print(f\"\\nüìú [{idx}/{len(prompt_files)}] Processing: {prompt_file} (Length: {prompt_length} characters)\")\n",
    "\n",
    "    # **Skip processing if the prompt is too long**\n",
    "    MAX_LENGTH = 100000  # Adjust this limit as needed\n",
    "    if prompt_length > MAX_LENGTH:\n",
    "        print(f\"‚ö†Ô∏è Skipping {prompt_file} because it exceeds {MAX_LENGTH} characters.\")\n",
    "        results.append([prompt_file, \"Skipped\", \"Prompt too long\"])\n",
    "        continue  # Skip to next prompt\n",
    "\n",
    "    # **Retry logic for invalid JSON responses**\n",
    "    retry_attempts = 3\n",
    "    for attempt in range(retry_attempts):\n",
    "        try:\n",
    "            response = openai.chat.completions.create(\n",
    "                model=LLM_NAME,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a cybersecurity expert analyzing software vulnerabilities. \"\n",
    "                                                  \"Provide your response **only in the following strict JSON format**:\\n\"\n",
    "                                                  \"{\\n\"\n",
    "                                                  '    \"answer\": \"Yes\" or \"No\",\\n'\n",
    "                                                  '    \"reasoning\": \"Concise but detailed explanation.\"\\n'\n",
    "                                                  \"}\\n\"\n",
    "                                                  \"Do not include any extra text outside this JSON structure.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt_content}\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            # Extract response\n",
    "            assistant_reply = response.choices[0].message.content.strip()\n",
    "\n",
    "            # ‚úÖ **Attempt to clean JSON formatting**\n",
    "            if assistant_reply.startswith(\"```json\"):\n",
    "                assistant_reply = assistant_reply[7:]  # Remove starting ```json\n",
    "            if assistant_reply.endswith(\"```\"):\n",
    "                assistant_reply = assistant_reply[:-3]  # Remove ending ```\n",
    "\n",
    "            try:\n",
    "                response_json = json.loads(assistant_reply)  # Ensure it's valid JSON\n",
    "                answer = response_json.get(\"answer\", \"N/A\")\n",
    "                reasoning = response_json.get(\"reasoning\", \"N/A\")\n",
    "\n",
    "                # Store results\n",
    "                results.append([prompt_file, answer, reasoning])\n",
    "                print(f\"‚úÖ Successfully processed {prompt_file}\")\n",
    "                break  # **Exit retry loop on success**\n",
    "\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"‚ö†Ô∏è Attempt {attempt + 1}/{retry_attempts}: Invalid JSON response for {prompt_file}, retrying...\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error processing {prompt_file}: {e}\")\n",
    "            error_logs.append([prompt_file, \"API Error\", str(e)])\n",
    "            results.append([prompt_file, \"Error\", str(e)])\n",
    "            break  # Stop retrying for API errors\n",
    "\n",
    "        # **Delay before retrying**\n",
    "        time.sleep(10)\n",
    "\n",
    "    else:\n",
    "        # If all retry attempts fail, log as invalid JSON\n",
    "        print(f\"‚ùå All {retry_attempts} attempts failed for {prompt_file}. Marking as error.\")\n",
    "        error_logs.append([prompt_file, \"Invalid JSON\", assistant_reply])\n",
    "        results.append([prompt_file, \"Error\", \"Invalid JSON response\"])\n",
    "\n",
    "    # **Longer delay to prevent API rate limits**\n",
    "    time.sleep(10)  # Adjust if needed\n",
    "\n",
    "# Convert results to DataFrame\n",
    "df_results = pd.DataFrame(results, columns=[\"Prompt File\", \"Answer\", \"Reasoning\"])\n",
    "\n",
    "# Save to Excel\n",
    "xlsx_path = os.path.join(RESULTS_DIR, f\"{LLM_NAME}_results.xlsx\")\n",
    "df_results.to_excel(xlsx_path, index=False)\n",
    "\n",
    "# Save error logs separately\n",
    "if error_logs:\n",
    "    df_errors = pd.DataFrame(error_logs, columns=[\"Prompt File\", \"Error Type\", \"Details\"])\n",
    "    error_xlsx_path = os.path.join(RESULTS_DIR, f\"{LLM_NAME}_errors.xlsx\")\n",
    "    df_errors.to_excel(error_xlsx_path, index=False)\n",
    "    print(f\"\\n‚ö†Ô∏è Errors saved separately in: {error_xlsx_path}\")\n",
    "\n",
    "print(f\"\\n‚úÖ All responses saved successfully in: {xlsx_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5818471d-8d96-4c8b-987a-bab1615aebc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03cccca6-ec0d-4baa-aa1b-b8e891f5a6bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìú [1/46] Processing: 1682_libxkbcommon_libxkbcommon_842e4351c2c97de6051cab6ce36b4a81e709a0e1_parser.c_changes_v9_v10.txt.txt (Length: 10295 characters)\n",
      "‚úÖ Successfully processed 1682_libxkbcommon_libxkbcommon_842e4351c2c97de6051cab6ce36b4a81e709a0e1_parser.c_changes_v9_v10.txt.txt\n",
      "\n",
      "üìú [2/46] Processing: 1697_libxml2_libxml2_899a5d9f0ed13b8e32449a08a361e0de127dd961_parser.c_changes_v342_v343.txt.txt (Length: 3175 characters)\n",
      "‚úÖ Successfully processed 1697_libxml2_libxml2_899a5d9f0ed13b8e32449a08a361e0de127dd961_parser.c_changes_v342_v343.txt.txt\n",
      "\n",
      "üìú [3/46] Processing: 1700_libzip_libzip_9b46957ec98d85a572e9ef98301247f39338a3b5_zip_open.c_changes_v6_v7.txt.txt (Length: 3189 characters)\n",
      "‚úÖ Successfully processed 1700_libzip_libzip_9b46957ec98d85a572e9ef98301247f39338a3b5_zip_open.c_changes_v6_v7.txt.txt\n",
      "\n",
      "üìú [4/46] Processing: 2464_linux_linux_1ee0a224bc9aad1de496c795f96bc6ba2c394811_io_ti.c_changes_v32_v33.txt.txt (Length: 15190 characters)\n",
      "‚úÖ Successfully processed 2464_linux_linux_1ee0a224bc9aad1de496c795f96bc6ba2c394811_io_ti.c_changes_v32_v33.txt.txt\n",
      "\n",
      "üìú [5/46] Processing: 7444_lxc_lxc_16af238036a5464ae8f2420ed3af214f0de875f9_lxc_user_nic.c_changes_v14_v15.txt.txt (Length: 7841 characters)\n",
      "‚úÖ Successfully processed 7444_lxc_lxc_16af238036a5464ae8f2420ed3af214f0de875f9_lxc_user_nic.c_changes_v14_v15.txt.txt\n",
      "\n",
      "üìú [6/46] Processing: 7462_lxcfs_lxcfs_8ee2a503e102b1a43ec4d83113dc275ab20a869a_lxcfs.c_changes_v31_v32.txt.txt (Length: 92273 characters)\n",
      "‚úÖ Successfully processed 7462_lxcfs_lxcfs_8ee2a503e102b1a43ec4d83113dc275ab20a869a_lxcfs.c_changes_v31_v32.txt.txt\n",
      "\n",
      "üìú [7/46] Processing: 7471_mapserver_mapserver_3a10f6b829297dae63492a8c63385044bc6953ed_mappostgis.c_changes_v52_v53.txt.txt (Length: 39431 characters)\n",
      "‚úÖ Successfully processed 7471_mapserver_mapserver_3a10f6b829297dae63492a8c63385044bc6953ed_mappostgis.c_changes_v52_v53.txt.txt\n",
      "\n",
      "üìú [8/46] Processing: 7481_mbedtls_mbedtls_a1098f81c252b317ad34ea978aea2bc47760b215_ssl_cli.c_changes_v27_v28.txt.txt (Length: 17820 characters)\n",
      "‚úÖ Successfully processed 7481_mbedtls_mbedtls_a1098f81c252b317ad34ea978aea2bc47760b215_ssl_cli.c_changes_v27_v28.txt.txt\n",
      "\n",
      "üìú [9/46] Processing: 7499_memcached_memcached_a8c4a82787b8b6c256d61bd5c42fb7f92d1bae00_memcached.c_changes_v120_v121.txt.txt (Length: 85319 characters)\n",
      "‚úÖ Successfully processed 7499_memcached_memcached_a8c4a82787b8b6c256d61bd5c42fb7f92d1bae00_memcached.c_changes_v120_v121.txt.txt\n",
      "\n",
      "üìú [10/46] Processing: 7503_miniupnp_miniupnp_13585f15c7f7dc28bbbba1661efb280d530d114c_upnpsoap.c_changes_v9_v10.txt.txt (Length: 3165 characters)\n",
      "‚úÖ Successfully processed 7503_miniupnp_miniupnp_13585f15c7f7dc28bbbba1661efb280d530d114c_upnpsoap.c_changes_v9_v10.txt.txt\n",
      "\n",
      "üìú [11/46] Processing: 7506_mruby_mruby_1905091634a6a2925c911484434448e568330626_vm.c_changes_v185_v186.txt.txt (Length: 5575 characters)\n",
      "‚úÖ Successfully processed 7506_mruby_mruby_1905091634a6a2925c911484434448e568330626_vm.c_changes_v185_v186.txt.txt\n",
      "\n",
      "üìú [12/46] Processing: 7526_nagioscore_nagioscore_1b197346d490df2e2d3b1dcce5ac6134ad0c8752_nagios.c_changes_v10_v11.txt.txt (Length: 4128 characters)\n",
      "‚úÖ Successfully processed 7526_nagioscore_nagioscore_1b197346d490df2e2d3b1dcce5ac6134ad0c8752_nagios.c_changes_v10_v11.txt.txt\n",
      "\n",
      "üìú [13/46] Processing: 7536_nbd_nbd_741495cb08503fd32a9d22648e63b64390c601f4_nbd-server.c_changes_v194_v195.txt.txt (Length: 9819 characters)\n",
      "‚úÖ Successfully processed 7536_nbd_nbd_741495cb08503fd32a9d22648e63b64390c601f4_nbd-server.c_changes_v194_v195.txt.txt\n",
      "\n",
      "üìú [14/46] Processing: 7542_neomutt_neomutt_3c49c44be9b459d9c616bcaef6eb5d51298c1741_command.c_changes_v162_v163.txt.txt (Length: 3919 characters)\n",
      "‚úÖ Successfully processed 7542_neomutt_neomutt_3c49c44be9b459d9c616bcaef6eb5d51298c1741_command.c_changes_v162_v163.txt.txt\n",
      "\n",
      "üìú [15/46] Processing: 7545_ngiflib_ngiflib_b588a2249c7abbfc52173e32ee11d6facef82f89_ngiflib.c_changes_v11_v12.txt.txt (Length: 3052 characters)\n",
      "‚úÖ Successfully processed 7545_ngiflib_ngiflib_b588a2249c7abbfc52173e32ee11d6facef82f89_ngiflib.c_changes_v11_v12.txt.txt\n",
      "\n",
      "üìú [16/46] Processing: 7552_oniguruma_oniguruma_9690d3ab1f9bcd2db8cbe1fe3ee4a5da606b8814_regexec.c_changes_v385_v386.txt.txt (Length: 3457 characters)\n",
      "‚úÖ Successfully processed 7552_oniguruma_oniguruma_9690d3ab1f9bcd2db8cbe1fe3ee4a5da606b8814_regexec.c_changes_v385_v386.txt.txt\n",
      "\n",
      "üìú [17/46] Processing: 7572_openjpeg_openjpeg_162f6199c0cd3ec1c6c6dc65e41b2faab92b2d91_color.c_changes_v3_v4.txt.txt (Length: 3191 characters)\n",
      "‚úÖ Successfully processed 7572_openjpeg_openjpeg_162f6199c0cd3ec1c6c6dc65e41b2faab92b2d91_color.c_changes_v3_v4.txt.txt\n",
      "\n",
      "üìú [18/46] Processing: 7591_openssl_openssl_103b171d8fc282ef435f8de9afbf7782e312961f_d1_pkt.c_changes_v13_v14.txt.txt (Length: 6353 characters)\n",
      "‚ö†Ô∏è Attempt 1/3: Invalid JSON response for 7591_openssl_openssl_103b171d8fc282ef435f8de9afbf7782e312961f_d1_pkt.c_changes_v13_v14.txt.txt, retrying...\n",
      "‚úÖ Successfully processed 7591_openssl_openssl_103b171d8fc282ef435f8de9afbf7782e312961f_d1_pkt.c_changes_v13_v14.txt.txt\n",
      "\n",
      "üìú [19/46] Processing: 7607_optee_os_optee_os_70697bf3c5dc3d201341b01a1a8e5bc6d2fb48f8_tee_svc_cryp.c_changes_v38_v39.txt.txt (Length: 20821 characters)\n",
      "‚úÖ Successfully processed 7607_optee_os_optee_os_70697bf3c5dc3d201341b01a1a8e5bc6d2fb48f8_tee_svc_cryp.c_changes_v38_v39.txt.txt\n",
      "\n",
      "üìú [20/46] Processing: 7624_pacemaker_pacemaker_5ec24a2642bd0854b884d1a9b51d12371373b410_tls_backend.c_changes_v3_v4.txt.txt (Length: 2950 characters)\n",
      "‚úÖ Successfully processed 7624_pacemaker_pacemaker_5ec24a2642bd0854b884d1a9b51d12371373b410_tls_backend.c_changes_v3_v4.txt.txt\n",
      "\n",
      "üìú [21/46] Processing: 7625_pango_pango_71aaeaf020340412b8d012fe23a556c0420eda5f_pango-emoji.c_changes_v2_v3.txt.txt (Length: 14208 characters)\n",
      "‚úÖ Successfully processed 7625_pango_pango_71aaeaf020340412b8d012fe23a556c0420eda5f_pango-emoji.c_changes_v2_v3.txt.txt\n",
      "\n",
      "üìú [22/46] Processing: 7626_pgbouncer_pgbouncer_edab5be6665b9e8de66c25ba527509b229468573_client.c_changes_v20_v21.txt.txt (Length: 5360 characters)\n",
      "‚úÖ Successfully processed 7626_pgbouncer_pgbouncer_edab5be6665b9e8de66c25ba527509b229468573_client.c_changes_v20_v21.txt.txt\n",
      "\n",
      "üìú [23/46] Processing: 7810_php-src_php-src_2938329ce19cb8c4197dec146c3ec887c6f61d01_gd.c_changes_v117_v118.txt.txt (Length: 122281 characters)\n",
      "‚ö†Ô∏è Skipping 7810_php-src_php-src_2938329ce19cb8c4197dec146c3ec887c6f61d01_gd.c_changes_v117_v118.txt.txt because it exceeds 100000 characters.\n",
      "\n",
      "üìú [24/46] Processing: 8572_picocom_picocom_1ebc60b20fbe9a02436d5cbbf8951714e749ddb1_picocom.c_changes_v45_v46.txt.txt (Length: 9082 characters)\n",
      "‚úÖ Successfully processed 8572_picocom_picocom_1ebc60b20fbe9a02436d5cbbf8951714e749ddb1_picocom.c_changes_v45_v46.txt.txt\n",
      "\n",
      "üìú [25/46] Processing: 8573_pigz_pigz_fdad1406b3ec809f4954ff7cdf9e99eb18c2458f_pigz.c_changes_v78_v79.txt.txt (Length: 10802 characters)\n",
      "‚úÖ Successfully processed 8573_pigz_pigz_fdad1406b3ec809f4954ff7cdf9e99eb18c2458f_pigz.c_changes_v78_v79.txt.txt\n",
      "\n",
      "üìú [26/46] Processing: 8574_pngquant_pngquant_b7c217680cda02dddced245d237ebe8c383be285_rwpng.c_changes_v9_v10.txt.txt (Length: 3818 characters)\n",
      "‚úÖ Successfully processed 8574_pngquant_pngquant_b7c217680cda02dddced245d237ebe8c383be285_rwpng.c_changes_v9_v10.txt.txt\n",
      "\n",
      "üìú [27/46] Processing: 8575_profanity_profanity_8e75437a7e43d4c55e861691f74892e666e29b0b_message.c_changes_v108_v109.txt.txt (Length: 51947 characters)\n",
      "‚úÖ Successfully processed 8575_profanity_profanity_8e75437a7e43d4c55e861691f74892e666e29b0b_message.c_changes_v108_v109.txt.txt\n",
      "\n",
      "üìú [28/46] Processing: 8581_qemu_qemu_103b40f51e4012b3b0ad20f615562a1806d7f49a_scsi-disk.c_changes_v97_v98.txt.txt (Length: 7180 characters)\n",
      "‚úÖ Successfully processed 8581_qemu_qemu_103b40f51e4012b3b0ad20f615562a1806d7f49a_scsi-disk.c_changes_v97_v98.txt.txt\n",
      "\n",
      "üìú [29/46] Processing: 8671_radare2_radare2_d1e8ac62c6d978d4662f69116e30230d43033c92_mach0.c_changes_v33_v34.txt.txt (Length: 9831 characters)\n",
      "‚úÖ Successfully processed 8671_radare2_radare2_d1e8ac62c6d978d4662f69116e30230d43033c92_mach0.c_changes_v33_v34.txt.txt\n",
      "\n",
      "üìú [30/46] Processing: 8704_redis_redis_1eb08bcd4634ae42ec45e8284923ac048beaa4c3_lua_struct.c_changes_v2_v3.txt.txt (Length: 8374 characters)\n",
      "‚úÖ Successfully processed 8704_redis_redis_1eb08bcd4634ae42ec45e8284923ac048beaa4c3_lua_struct.c_changes_v2_v3.txt.txt\n",
      "\n",
      "üìú [31/46] Processing: 8707_rpm_rpm_404ef011c300207cdb1e531670384564aae04bdc_fsm.c_changes_v24_v25.txt.txt (Length: 11901 characters)\n",
      "‚úÖ Successfully processed 8707_rpm_rpm_404ef011c300207cdb1e531670384564aae04bdc_fsm.c_changes_v24_v25.txt.txt\n",
      "\n",
      "üìú [32/46] Processing: 8713_sgminer_sgminer_78cc408369bdbbd440196c93574098d1482efbce_util.c_changes_v2_v3.txt.txt (Length: 4223 characters)\n",
      "‚úÖ Successfully processed 8713_sgminer_sgminer_78cc408369bdbbd440196c93574098d1482efbce_util.c_changes_v2_v3.txt.txt\n",
      "\n",
      "üìú [33/46] Processing: 8714_sleuthkit_sleuthkit_114cd3d0aac8bd1aeaf4b33840feb0163d342d5b_hfs.c_changes_v2_v3.txt.txt (Length: 23432 characters)\n",
      "‚úÖ Successfully processed 8714_sleuthkit_sleuthkit_114cd3d0aac8bd1aeaf4b33840feb0163d342d5b_hfs.c_changes_v2_v3.txt.txt\n",
      "\n",
      "üìú [34/46] Processing: 8721_slurm_slurm_92362a92fffe60187df61f99ab11c249d44120ee_req.c_changes_v110_v111.txt.txt (Length: 31664 characters)\n",
      "‚úÖ Successfully processed 8721_slurm_slurm_92362a92fffe60187df61f99ab11c249d44120ee_req.c_changes_v110_v111.txt.txt\n",
      "\n",
      "üìú [35/46] Processing: 8802_src_src_ac8147a06ed2e2403fb6b9a0c03e618a9333c0e9_authfile.c_changes_v19_v20.txt.txt (Length: 4592 characters)\n",
      "‚úÖ Successfully processed 8802_src_src_ac8147a06ed2e2403fb6b9a0c03e618a9333c0e9_authfile.c_changes_v19_v20.txt.txt\n",
      "\n",
      "üìú [36/46] Processing: 8805_systemd_systemd_cb31827d62066a04b02111df3052949fda4b6888_nss-mymachines.c_changes_v9_v10.txt.txt (Length: 3691 characters)\n",
      "‚úÖ Successfully processed 8805_systemd_systemd_cb31827d62066a04b02111df3052949fda4b6888_nss-mymachines.c_changes_v9_v10.txt.txt\n",
      "\n",
      "üìú [37/46] Processing: 8806_tcpdump_tcpdump_0f95d441e4b5d7512cc5c326c8668a120e048eda_print-ppp.c_changes_v49_v50.txt.txt (Length: 3909 characters)\n",
      "‚úÖ Successfully processed 8806_tcpdump_tcpdump_0f95d441e4b5d7512cc5c326c8668a120e048eda_print-ppp.c_changes_v49_v50.txt.txt\n",
      "\n",
      "üìú [38/46] Processing: 8864_tcpreplay_tcpreplay_d689d14dbcd768c028eab2fb378d849e543dcfe9_tcpcapinfo.c_changes_v7_v8.txt.txt (Length: 2899 characters)\n",
      "‚úÖ Successfully processed 8864_tcpreplay_tcpreplay_d689d14dbcd768c028eab2fb378d849e543dcfe9_tcpcapinfo.c_changes_v7_v8.txt.txt\n",
      "\n",
      "üìú [39/46] Processing: 8892_tor_tor_56a7c5bc15e0447203a491c1ee37de9939ad1dcd_relay.c_changes_v66_v67.txt.txt (Length: 57215 characters)\n",
      "‚úÖ Successfully processed 8892_tor_tor_56a7c5bc15e0447203a491c1ee37de9939ad1dcd_relay.c_changes_v66_v67.txt.txt\n",
      "\n",
      "üìú [40/46] Processing: 8897_unrealircd_unrealircd_f473e355e1dc422c4f019dbf86bc50ba1a34a766_m_sasl.c_changes_v7_v8.txt.txt (Length: 4055 characters)\n",
      "‚úÖ Successfully processed 8897_unrealircd_unrealircd_f473e355e1dc422c4f019dbf86bc50ba1a34a766_m_sasl.c_changes_v7_v8.txt.txt\n",
      "\n",
      "üìú [41/46] Processing: 8898_uriparser_uriparser_f76275d4a91b28d687250525d3a0c5509bbd666f_UriQuery.c_changes_v10_v11.txt.txt (Length: 3803 characters)\n",
      "‚úÖ Successfully processed 8898_uriparser_uriparser_f76275d4a91b28d687250525d3a0c5509bbd666f_UriQuery.c_changes_v10_v11.txt.txt\n",
      "\n",
      "üìú [42/46] Processing: 8901_util-linux_util-linux_dffab154d29a288aa171ff50263ecc8f2e14a891_su-common.c_changes_v9_v10.txt.txt (Length: 44998 characters)\n",
      "‚úÖ Successfully processed 8901_util-linux_util-linux_dffab154d29a288aa171ff50263ecc8f2e14a891_su-common.c_changes_v9_v10.txt.txt\n",
      "\n",
      "üìú [43/46] Processing: 8902_wildmidi_wildmidi_814f31d8eceda8401eb812fc2e94ed143fdad0ab_wildmidi_lib.c_changes_v17_v18.txt.txt (Length: 28569 characters)\n",
      "‚úÖ Successfully processed 8902_wildmidi_wildmidi_814f31d8eceda8401eb812fc2e94ed143fdad0ab_wildmidi_lib.c_changes_v17_v18.txt.txt\n",
      "\n",
      "üìú [44/46] Processing: 8904_wireshark_wireshark_5efb45231671baa2db2011d8f67f9d6e72bc455b_toshiba.c_changes_v2_v3.txt.txt (Length: 5489 characters)\n",
      "‚úÖ Successfully processed 8904_wireshark_wireshark_5efb45231671baa2db2011d8f67f9d6e72bc455b_toshiba.c_changes_v2_v3.txt.txt\n",
      "\n",
      "üìú [45/46] Processing: 8912_wolfssl_wolfssl_9b9568d500f31f964af26ba8d01e542e1f27e5ca_ecc.c_changes_v116_v117.txt.txt (Length: 18284 characters)\n",
      "‚úÖ Successfully processed 8912_wolfssl_wolfssl_9b9568d500f31f964af26ba8d01e542e1f27e5ca_ecc.c_changes_v116_v117.txt.txt\n",
      "\n",
      "üìú [46/46] Processing: 8985_yara_yara_890c3f850293176c0e996a602ffa88b315f4e98f_grammar.c_changes_v113_v114.txt.txt (Length: 88843 characters)\n",
      "‚úÖ Successfully processed 8985_yara_yara_890c3f850293176c0e996a602ffa88b315f4e98f_grammar.c_changes_v113_v114.txt.txt\n",
      "\n",
      "‚úÖ All responses saved successfully in: M:\\FULL_DATA_COLLECTED\\newDiverseSample\\gpt-4o\\gpt-4o_resultsTempSetting.xlsx\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import openai\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "import chardet  # Detects file encoding\n",
    "\n",
    "# Define paths\n",
    "PROMPT_DIR = r\"M:\\FULL_DATA_COLLECTED\\newDiverseSample\"\n",
    "LLM_NAME = \"gpt-4o\"  # Change this if using a different LLM\n",
    "RESULTS_DIR = os.path.join(PROMPT_DIR, LLM_NAME)\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)  # Ensure directory exists\n",
    "\n",
    "# Prepare storage for results\n",
    "results = []\n",
    "error_logs = []\n",
    "\n",
    "# List all prompt files\n",
    "prompt_files = [f for f in os.listdir(PROMPT_DIR) if f.endswith(\".txt\")]\n",
    "\n",
    "for idx, prompt_file in enumerate(prompt_files, start=1):\n",
    "    prompt_path = os.path.join(PROMPT_DIR, prompt_file)\n",
    "\n",
    "    # Detect file encoding\n",
    "    with open(prompt_path, \"rb\") as f:\n",
    "        raw_data = f.read()\n",
    "        encoding = chardet.detect(raw_data)['encoding']\n",
    "    \n",
    "    # Read the prompt content with detected encoding\n",
    "    try:\n",
    "        with open(prompt_path, \"r\", encoding=encoding) as f:\n",
    "            prompt_content = f.read().strip()\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Encoding issue with {prompt_file}: {e}\")\n",
    "        error_logs.append([prompt_file, \"Encoding Error\", str(e)])\n",
    "        continue  # Skip this file and move to next\n",
    "\n",
    "    # Print prompt details\n",
    "    prompt_length = len(prompt_content)\n",
    "    print(f\"\\nüìú [{idx}/{len(prompt_files)}] Processing: {prompt_file} (Length: {prompt_length} characters)\")\n",
    "\n",
    "    # **Skip processing if the prompt is too long**\n",
    "    MAX_LENGTH = 100000  # Adjust this limit as needed\n",
    "    if prompt_length > MAX_LENGTH:\n",
    "        print(f\"‚ö†Ô∏è Skipping {prompt_file} because it exceeds {MAX_LENGTH} characters.\")\n",
    "        results.append([prompt_file, \"Skipped\", \"Prompt too long\"])\n",
    "        continue  # Skip to next prompt\n",
    "\n",
    "    # **Retry logic for invalid JSON responses**\n",
    "    retry_attempts = 3\n",
    "    for attempt in range(retry_attempts):\n",
    "        try:\n",
    "            response = openai.chat.completions.create(\n",
    "                model=LLM_NAME,\n",
    "                temperature=0.3,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a cybersecurity expert analyzing software vulnerabilities. \"\n",
    "                                                  \"Provide your response **only in the following strict JSON format**:\\n\"\n",
    "                                                  \"{\\n\"\n",
    "                                                  '    \"answer\": \"Yes\" or \"No\",\\n'\n",
    "                                                  '    \"reasoning\": \"Concise but detailed explanation.\"\\n'\n",
    "                                                  \"}\\n\"\n",
    "                                                  \"Do not include any extra text outside this JSON structure.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt_content}\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            # Extract response\n",
    "            assistant_reply = response.choices[0].message.content.strip()\n",
    "\n",
    "            # ‚úÖ **Attempt to clean JSON formatting**\n",
    "            if assistant_reply.startswith(\"```json\"):\n",
    "                assistant_reply = assistant_reply[7:]  # Remove starting ```json\n",
    "            if assistant_reply.endswith(\"```\"):\n",
    "                assistant_reply = assistant_reply[:-3]  # Remove ending ```\n",
    "\n",
    "            try:\n",
    "                response_json = json.loads(assistant_reply)  # Ensure it's valid JSON\n",
    "                answer = response_json.get(\"answer\", \"N/A\")\n",
    "                reasoning = response_json.get(\"reasoning\", \"N/A\")\n",
    "\n",
    "                # Store results\n",
    "                results.append([prompt_file, answer, reasoning])\n",
    "                print(f\"‚úÖ Successfully processed {prompt_file}\")\n",
    "                break  # **Exit retry loop on success**\n",
    "\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"‚ö†Ô∏è Attempt {attempt + 1}/{retry_attempts}: Invalid JSON response for {prompt_file}, retrying...\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error processing {prompt_file}: {e}\")\n",
    "            error_logs.append([prompt_file, \"API Error\", str(e)])\n",
    "            results.append([prompt_file, \"Error\", str(e)])\n",
    "            break  # Stop retrying for API errors\n",
    "\n",
    "        # **Delay before retrying**\n",
    "        time.sleep(10)\n",
    "\n",
    "    else:\n",
    "        # If all retry attempts fail, log as invalid JSON\n",
    "        print(f\"‚ùå All {retry_attempts} attempts failed for {prompt_file}. Marking as error.\")\n",
    "        error_logs.append([prompt_file, \"Invalid JSON\", assistant_reply])\n",
    "        results.append([prompt_file, \"Error\", \"Invalid JSON response\"])\n",
    "\n",
    "    # **Longer delay to prevent API rate limits**\n",
    "    time.sleep(10)  # Adjust if needed\n",
    "\n",
    "# Convert results to DataFrame\n",
    "df_results = pd.DataFrame(results, columns=[\"Prompt File\", \"Answer\", \"Reasoning\"])\n",
    "\n",
    "# Save to Excel\n",
    "xlsx_path = os.path.join(RESULTS_DIR, f\"{LLM_NAME}_resultsTempSetting.xlsx\")\n",
    "df_results.to_excel(xlsx_path, index=False)\n",
    "\n",
    "# Save error logs separately\n",
    "if error_logs:\n",
    "    df_errors = pd.DataFrame(error_logs, columns=[\"Prompt File\", \"Error Type\", \"Details\"])\n",
    "    error_xlsx_path = os.path.join(RESULTS_DIR, f\"{LLM_NAME}_errorsTempSetting.xlsx\")\n",
    "    df_errors.to_excel(error_xlsx_path, index=False)\n",
    "    print(f\"\\n‚ö†Ô∏è Errors saved separately in: {error_xlsx_path}\")\n",
    "\n",
    "print(f\"\\n‚úÖ All responses saved successfully in: {xlsx_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240e24e5-b925-430b-9886-3c423acedbab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all prompt range wise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa10574-0baf-4b24-8b96-fdecd0355903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Processed 8750 records so far...\n",
      "‚úÖ Processed 8800 records so far...\n",
      "‚úÖ Processed 8850 records so far...\n",
      "‚úÖ Processed 8900 records so far...\n",
      "‚úÖ Processed 8950 records so far...\n",
      "‚úÖ Processed 9000 records so far...\n",
      "‚úÖ Processed 9050 records so far...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import openai\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "import chardet  # Detects file encoding\n",
    "import re  # For extracting numerical ranges\n",
    "\n",
    "# Initialize OpenAI client (‚úÖ New API format)\n",
    "client = openai.Client(api_key=\"key\")  # Replace with actual API key\n",
    "\n",
    "# Define paths\n",
    "PROMPT_DIR = r\"M:\\FULL_DATA_COLLECTED\\PromptsFull\"\n",
    "LLM_NAME = \"gpt-4o\"\n",
    "\n",
    "# üü¢ **SET YOUR RANGE HERE (Manually update this)**\n",
    "RANGE_INPUT = \"8701-9057\"  # Example: \"100-200\", \"200-n\"\n",
    "\n",
    "# Create a directory for storing results inside PromptsFull\n",
    "RESULTS_DIR = os.path.join(PROMPT_DIR, LLM_NAME)\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)  # Ensure directory exists\n",
    "\n",
    "# Function to extract range\n",
    "def extract_range(range_str):\n",
    "    match = re.match(r\"(\\d+)-(\\d+|n)\", range_str)\n",
    "    if match:\n",
    "        start, end = match.groups()\n",
    "        start = int(start)\n",
    "        end = int(end) if end.isdigit() else None  # `None` means all remaining\n",
    "        return start, end\n",
    "    return None, None\n",
    "\n",
    "start_idx, end_idx = extract_range(RANGE_INPUT)\n",
    "\n",
    "# Prepare storage for results\n",
    "results = []\n",
    "error_logs = []\n",
    "\n",
    "# List all prompt files, sorted numerically\n",
    "prompt_files = sorted(\n",
    "    [f for f in os.listdir(PROMPT_DIR) if f.endswith(\".txt\")],\n",
    "    key=lambda x: int(re.search(r'\\d+', x).group()) if re.search(r'\\d+', x) else float('inf')\n",
    ")\n",
    "\n",
    "# Filter files based on manually set range\n",
    "if start_idx is not None:\n",
    "    prompt_files = prompt_files[start_idx - 1:end_idx]  # Adjust for 0-based index\n",
    "\n",
    "for idx, prompt_file in enumerate(prompt_files, start=start_idx):\n",
    "    prompt_path = os.path.join(PROMPT_DIR, prompt_file)\n",
    "\n",
    "    # Detect file encoding\n",
    "    with open(prompt_path, \"rb\") as f:\n",
    "        raw_data = f.read()\n",
    "        encoding = chardet.detect(raw_data)['encoding']\n",
    "\n",
    "    # Read the prompt content with detected encoding\n",
    "    try:\n",
    "        with open(prompt_path, \"r\", encoding=encoding) as f:\n",
    "            prompt_content = f.read().strip()\n",
    "    except Exception as e:\n",
    "        error_logs.append([prompt_file, \"Encoding Error\", str(e)])\n",
    "        continue\n",
    "\n",
    "    # Retry mechanism (up to 3 times)\n",
    "    retries = 3\n",
    "    for attempt in range(1, retries + 1):\n",
    "        try:\n",
    "            response = client.chat.completions.create(  # ‚úÖ Updated API call\n",
    "                model=LLM_NAME,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a cybersecurity expert analyzing software vulnerabilities. \"\n",
    "                                                  \"Provide your response **only in the following strict JSON format**:\\n\"\n",
    "                                                  \"{\\n\"\n",
    "                                                  '    \"answer\": \"Yes\" or \"No\",\\n'\n",
    "                                                  '    \"reasoning\": \"Concise but detailed explanation.\",\\n'\n",
    "                                                  '    \"confidence\": 1-10\\n'\n",
    "                                                  \"}\\n\"\n",
    "                                                  \"Do not include any extra text outside this JSON structure.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt_content}\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            # ‚úÖ Handle markdown output\n",
    "            assistant_reply = response.choices[0].message.content.strip()\n",
    "            if assistant_reply.startswith(\"```json\"):\n",
    "                assistant_reply = assistant_reply[7:]\n",
    "            if assistant_reply.endswith(\"```\"):\n",
    "                assistant_reply = assistant_reply[:-3]\n",
    "\n",
    "            # ‚úÖ Parse JSON response\n",
    "            try:\n",
    "                response_json = json.loads(assistant_reply)\n",
    "                answer = response_json.get(\"answer\", \"N/A\")\n",
    "                reasoning = response_json.get(\"reasoning\", \"N/A\")\n",
    "                confidence = response_json.get(\"confidence\", \"N/A\")  # ‚úÖ Extract confidence\n",
    "\n",
    "                # Store results\n",
    "                results.append([prompt_file, answer, reasoning, confidence])\n",
    "                break  # **Exit retry loop on success**\n",
    "\n",
    "            except json.JSONDecodeError:\n",
    "                error_logs.append([prompt_file, \"Invalid JSON\", assistant_reply])\n",
    "\n",
    "        except openai.RateLimitError:\n",
    "            time.sleep(120)  # ‚úÖ Increased wait time to 120 seconds\n",
    "\n",
    "        except openai.APIError as e:\n",
    "            error_logs.append([prompt_file, \"API Error\", str(e)])\n",
    "\n",
    "        except Exception as e:\n",
    "            error_logs.append([prompt_file, \"Unexpected Error\", str(e)])\n",
    "\n",
    "        if attempt == retries:\n",
    "            results.append([prompt_file, \"Error\", \"Final failure after retries\", \"N/A\"])\n",
    "\n",
    "    # **Print progress every 50 records**\n",
    "    if idx % 50 == 0:\n",
    "        print(f\"‚úÖ Processed {idx} records so far...\")\n",
    "\n",
    "    # **Longer delay to prevent API rate limits**\n",
    "    time.sleep(10)  # Adjust if needed\n",
    "\n",
    "# Convert results to DataFrame\n",
    "df_results = pd.DataFrame(results, columns=[\"Prompt File\", \"Answer\", \"Reasoning\", \"Confidence\"])\n",
    "\n",
    "# Define filename based on range\n",
    "xlsx_filename = f\"{LLM_NAME}_results_{RANGE_INPUT.replace('-', '_')}.xlsx\"\n",
    "xlsx_path = os.path.join(RESULTS_DIR, xlsx_filename)\n",
    "df_results.to_excel(xlsx_path, index=False)\n",
    "\n",
    "# Save error logs separately\n",
    "if error_logs:\n",
    "    error_xlsx_filename = f\"{LLM_NAME}_errors_{RANGE_INPUT.replace('-', '_')}.xlsx\"\n",
    "    error_xlsx_path = os.path.join(RESULTS_DIR, error_xlsx_filename)\n",
    "    df_errors = pd.DataFrame(error_logs, columns=[\"Prompt File\", \"Error Type\", \"Details\"])\n",
    "    df_errors.to_excel(error_xlsx_path, index=False)\n",
    "\n",
    "print(f\"\\n‚úÖ Successfully processed all files in range {RANGE_INPUT}. Results saved in: {xlsx_path}\")\n",
    "if error_logs:\n",
    "    print(f\"‚ö†Ô∏è Errors logged in: {error_xlsx_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d557a8cb-bf0f-4d05-b97e-8454135e3486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetching functionality, semantics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a8ea4f7-8fbf-4f05-8c21-64d5d702b105",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Processed 150 files...\n",
      "‚úÖ Processed 200 files...\n",
      "‚úÖ Processed 250 files...\n",
      "‚úÖ Processed 300 files...\n",
      "‚úÖ Processed 350 files...\n",
      "\n",
      "‚úÖ Finished processing range 101-390.\n",
      "üìù Results saved to: M:\\FULL_DATA_COLLECTED\\FINAL_DATASET\\prompts2\\gpt-4o_semantic_summaries_101_390.xlsx\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import openai\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "import chardet\n",
    "import re\n",
    "\n",
    "# === OpenAI client setup ===\n",
    "client = openai.Client(api_key=\"key\")  # Replace with your actual key\n",
    "LLM_NAME = \"gpt-4o\"\n",
    "\n",
    "# === Directories and range config ===\n",
    "PROMPT_DIR = r\"M:\\FULL_DATA_COLLECTED\\FINAL_DATASET\\prompts2\"\n",
    "RESULTS_DIR = PROMPT_DIR  # Save output in same directory\n",
    "\n",
    "RANGE_INPUT = \"101-390\"  # Update this range manually\n",
    "\n",
    "# === Extract range ===\n",
    "def extract_range(range_str):\n",
    "    match = re.match(r\"(\\d+)-(\\d+|n)\", range_str)\n",
    "    if match:\n",
    "        start, end = match.groups()\n",
    "        start = int(start)\n",
    "        end = int(end) if end.isdigit() else None\n",
    "        return start, end\n",
    "    return None, None\n",
    "\n",
    "start_idx, end_idx = extract_range(RANGE_INPUT)\n",
    "\n",
    "# === List and filter prompt files ===\n",
    "prompt_files = sorted(\n",
    "    [f for f in os.listdir(PROMPT_DIR) if f.endswith(\".txt\")],\n",
    "    key=lambda x: int(re.search(r'\\d+', x).group()) if re.search(r'\\d+', x) else float('inf')\n",
    ")\n",
    "\n",
    "if start_idx is not None:\n",
    "    prompt_files = prompt_files[start_idx - 1:end_idx]\n",
    "\n",
    "# === Storage for outputs and errors ===\n",
    "results = []\n",
    "error_logs = []\n",
    "\n",
    "# === Process each prompt ===\n",
    "for idx, prompt_file in enumerate(prompt_files, start=start_idx):\n",
    "    prompt_path = os.path.join(PROMPT_DIR, prompt_file)\n",
    "\n",
    "    # Detect encoding\n",
    "    with open(prompt_path, \"rb\") as f:\n",
    "        raw_data = f.read()\n",
    "        encoding = chardet.detect(raw_data)['encoding']\n",
    "\n",
    "    try:\n",
    "        with open(prompt_path, \"r\", encoding=encoding) as f:\n",
    "            prompt_content = f.read().strip()\n",
    "    except Exception as e:\n",
    "        error_logs.append([prompt_file, \"Encoding Error\", str(e)])\n",
    "        continue\n",
    "\n",
    "    # === Send to OpenAI with retries ===\n",
    "    retries = 3\n",
    "    for attempt in range(1, retries + 1):\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=LLM_NAME,\n",
    "                messages=[\n",
    "                    {\"role\": \"user\", \"content\": prompt_content}\n",
    "                ]\n",
    "            )\n",
    "            summary = response.choices[0].message.content.strip()\n",
    "            results.append([prompt_file, summary])\n",
    "            break  # ‚úÖ Success\n",
    "\n",
    "        except openai.RateLimitError:\n",
    "            time.sleep(120)\n",
    "\n",
    "        except openai.APIError as e:\n",
    "            error_logs.append([prompt_file, \"API Error\", str(e)])\n",
    "\n",
    "        except Exception as e:\n",
    "            error_logs.append([prompt_file, \"Unexpected Error\", str(e)])\n",
    "\n",
    "        if attempt == retries:\n",
    "            results.append([prompt_file, \"ERROR: failed after 3 retries\"])\n",
    "\n",
    "    # === Print progress every 50 ===\n",
    "    if idx % 50 == 0:\n",
    "        print(f\"‚úÖ Processed {idx} files...\")\n",
    "\n",
    "    time.sleep(10)  # Adjust if needed to avoid rate limits\n",
    "\n",
    "# === Save summary results ===\n",
    "df_results = pd.DataFrame(results, columns=[\"Prompt File\", \"Semantic Summary\"])\n",
    "filename_out = f\"{LLM_NAME}_semantic_summaries_{RANGE_INPUT.replace('-', '_')}.xlsx\"\n",
    "df_results.to_excel(os.path.join(RESULTS_DIR, filename_out), index=False)\n",
    "\n",
    "# === Save error logs if any ===\n",
    "if error_logs:\n",
    "    df_errors = pd.DataFrame(error_logs, columns=[\"Prompt File\", \"Error Type\", \"Details\"])\n",
    "    filename_err = f\"{LLM_NAME}_errors_{RANGE_INPUT.replace('-', '_')}.xlsx\"\n",
    "    df_errors.to_excel(os.path.join(RESULTS_DIR, filename_err), index=False)\n",
    "\n",
    "# === Final output ===\n",
    "print(f\"\\n‚úÖ Finished processing range {RANGE_INPUT}.\")\n",
    "print(f\"üìù Results saved to: {os.path.join(RESULTS_DIR, filename_out)}\")\n",
    "if error_logs:\n",
    "    print(f\"‚ö†Ô∏è Errors saved to: {os.path.join(RESULTS_DIR, filename_err)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44376935-ae4b-45bd-b1ac-b44ff1829768",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge GPT responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fcae5f9a-fae0-41a8-9836-0677815ba7a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded: gpt-4o_semantic_summaries_1_10.xlsx\n",
      "‚úÖ Loaded: gpt-4o_semantic_summaries_11_100.xlsx\n",
      "‚úÖ Loaded: gpt-4o_semantic_summaries_101_390.xlsx\n",
      "\n",
      "‚úÖ Merged 3 files into: M:\\FULL_DATA_COLLECTED\\FINAL_DATASET\\prompts2\\gpt-4o_semantic_summaries_merged.xlsx\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Directory where your range-based files are stored\n",
    "RESULTS_DIR = r\"M:\\FULL_DATA_COLLECTED\\FINAL_DATASET\\prompts2\"\n",
    "\n",
    "# Output filename\n",
    "merged_filename = \"gpt-4o_semantic_summaries_merged.xlsx\"\n",
    "merged_path = os.path.join(RESULTS_DIR, merged_filename)\n",
    "\n",
    "# Find all matching summary files (excluding 'merged')\n",
    "summary_files = [\n",
    "    f for f in os.listdir(RESULTS_DIR)\n",
    "    if f.startswith(\"gpt-4o_semantic_summaries_\") and f.endswith(\".xlsx\") and \"merged\" not in f\n",
    "]\n",
    "\n",
    "# Extract starting index from filenames for sorting\n",
    "def extract_start_index(filename):\n",
    "    match = re.search(r\"gpt-4o_semantic_summaries_(\\d+)_\\d+\\.xlsx\", filename)\n",
    "    return int(match.group(1)) if match else float('inf')\n",
    "\n",
    "# Sort files based on starting index of their range\n",
    "summary_files_sorted = sorted(summary_files, key=extract_start_index)\n",
    "\n",
    "# Merge all into one DataFrame\n",
    "df_list = []\n",
    "for file in summary_files_sorted:\n",
    "    path = os.path.join(RESULTS_DIR, file)\n",
    "    try:\n",
    "        df = pd.read_excel(path)\n",
    "        df_list.append(df)\n",
    "        print(f\"‚úÖ Loaded: {file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Skipped {file} due to error: {e}\")\n",
    "\n",
    "# Save merged file\n",
    "if df_list:\n",
    "    merged_df = pd.concat(df_list, ignore_index=True)\n",
    "    merged_df.to_excel(merged_path, index=False)\n",
    "    print(f\"\\n‚úÖ Merged {len(df_list)} files into: {merged_path}\")\n",
    "else:\n",
    "    print(\"‚ùå No matching summary files found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32598f67-161a-454c-9997-f355305f1c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for no responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e0bed0ef-ff5a-407e-9cbd-bd3a7b50c651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Processed 250 files...\n",
      "‚úÖ Processed 300 files...\n",
      "‚úÖ Processed 350 files...\n",
      "‚úÖ Processed 400 files...\n",
      "\n",
      "‚úÖ Finished processing range 201-432.\n",
      "üìù Results saved to: M:\\FULL_DATA_COLLECTED\\FINAL_DATASET\\no\\prompts2\\gpt-4o_semantic_summaries_201_432_no.xlsx\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import openai\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "import chardet\n",
    "import re\n",
    "\n",
    "# === OpenAI client setup ===\n",
    "client = openai.Client(api_key=\"key\")  # Replace with your actual key\n",
    "LLM_NAME = \"gpt-4o\"\n",
    "\n",
    "# === Directories and range config ===\n",
    "PROMPT_DIR = r\"M:\\FULL_DATA_COLLECTED\\FINAL_DATASET\\no\\prompts2\"\n",
    "RESULTS_DIR = PROMPT_DIR  # Save output in same directory\n",
    "\n",
    "RANGE_INPUT = \"201-432\"  # Update this range manually as needed\n",
    "\n",
    "# === Extract range ===\n",
    "def extract_range(range_str):\n",
    "    match = re.match(r\"(\\d+)-(\\d+|n)\", range_str)\n",
    "    if match:\n",
    "        start, end = match.groups()\n",
    "        start = int(start)\n",
    "        end = int(end) if end.isdigit() else None\n",
    "        return start, end\n",
    "    return None, None\n",
    "\n",
    "start_idx, end_idx = extract_range(RANGE_INPUT)\n",
    "\n",
    "# === List and filter prompt files ===\n",
    "prompt_files = sorted(\n",
    "    [f for f in os.listdir(PROMPT_DIR) if f.endswith(\".txt\")],\n",
    "    key=lambda x: int(re.search(r'\\d+', x).group()) if re.search(r'\\d+', x) else float('inf')\n",
    ")\n",
    "\n",
    "if start_idx is not None:\n",
    "    prompt_files = prompt_files[start_idx - 1:end_idx]\n",
    "\n",
    "# === Storage for outputs and errors ===\n",
    "results = []\n",
    "error_logs = []\n",
    "\n",
    "# === Process each prompt ===\n",
    "for idx, prompt_file in enumerate(prompt_files, start=start_idx):\n",
    "    prompt_path = os.path.join(PROMPT_DIR, prompt_file)\n",
    "\n",
    "    # Detect encoding\n",
    "    with open(prompt_path, \"rb\") as f:\n",
    "        raw_data = f.read()\n",
    "        encoding = chardet.detect(raw_data)['encoding']\n",
    "\n",
    "    try:\n",
    "        with open(prompt_path, \"r\", encoding=encoding) as f:\n",
    "            prompt_content = f.read().strip()\n",
    "    except Exception as e:\n",
    "        error_logs.append([prompt_file, \"Encoding Error\", str(e)])\n",
    "        continue\n",
    "\n",
    "    # === Send to OpenAI with retries ===\n",
    "    retries = 3\n",
    "    for attempt in range(1, retries + 1):\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=LLM_NAME,\n",
    "                messages=[\n",
    "                    {\"role\": \"user\", \"content\": prompt_content}\n",
    "                ]\n",
    "            )\n",
    "            summary = response.choices[0].message.content.strip()\n",
    "            results.append([prompt_file, summary])\n",
    "            break  # ‚úÖ Success\n",
    "\n",
    "        except openai.RateLimitError:\n",
    "            time.sleep(120)\n",
    "\n",
    "        except openai.APIError as e:\n",
    "            error_logs.append([prompt_file, \"API Error\", str(e)])\n",
    "\n",
    "        except Exception as e:\n",
    "            error_logs.append([prompt_file, \"Unexpected Error\", str(e)])\n",
    "\n",
    "        if attempt == retries:\n",
    "            results.append([prompt_file, \"ERROR: failed after 3 retries\"])\n",
    "\n",
    "    # === Print progress every 50 ===\n",
    "    if idx % 50 == 0:\n",
    "        print(f\"‚úÖ Processed {idx} files...\")\n",
    "\n",
    "    time.sleep(10)  # Adjust if needed to avoid rate limits\n",
    "\n",
    "# === Save summary results ===\n",
    "df_results = pd.DataFrame(results, columns=[\"Prompt File\", \"Semantic Summary\"])\n",
    "filename_out = f\"{LLM_NAME}_semantic_summaries_{RANGE_INPUT.replace('-', '_')}_no.xlsx\"\n",
    "df_results.to_excel(os.path.join(RESULTS_DIR, filename_out), index=False)\n",
    "\n",
    "# === Save error logs if any ===\n",
    "if error_logs:\n",
    "    df_errors = pd.DataFrame(error_logs, columns=[\"Prompt File\", \"Error Type\", \"Details\"])\n",
    "    filename_err = f\"{LLM_NAME}_errors_{RANGE_INPUT.replace('-', '_')}_no.xlsx\"\n",
    "    df_errors.to_excel(os.path.join(RESULTS_DIR, filename_err), index=False)\n",
    "\n",
    "# === Final output ===\n",
    "print(f\"\\n‚úÖ Finished processing range {RANGE_INPUT}.\")\n",
    "print(f\"üìù Results saved to: {os.path.join(RESULTS_DIR, filename_out)}\")\n",
    "if error_logs:\n",
    "    print(f\"‚ö†Ô∏è Errors saved to: {os.path.join(RESULTS_DIR, filename_err)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f10520-df69-4341-b5e8-50c5919d60cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge outputs for \"No\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5c5f4cbd-2c6f-4a20-b579-f9f961494422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded: gpt-4o_semantic_summaries_1_10_no.xlsx\n",
      "‚úÖ Loaded: gpt-4o_semantic_summaries_11_200_no.xlsx\n",
      "‚úÖ Loaded: gpt-4o_semantic_summaries_201_432_no.xlsx\n",
      "\n",
      "‚úÖ Merged 3 files into: M:\\FULL_DATA_COLLECTED\\FINAL_DATASET\\no\\prompts2\\gpt-4o_semantic_summaries_merged_no.xlsx\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Directory where \"no\" response files are stored\n",
    "RESULTS_DIR = r\"M:\\FULL_DATA_COLLECTED\\FINAL_DATASET\\no\\prompts2\"\n",
    "\n",
    "# Output filename\n",
    "merged_filename = \"gpt-4o_semantic_summaries_merged_no.xlsx\"\n",
    "merged_path = os.path.join(RESULTS_DIR, merged_filename)\n",
    "\n",
    "# Find all matching summary files for \"no\" responses (excluding merged)\n",
    "summary_files = [\n",
    "    f for f in os.listdir(RESULTS_DIR)\n",
    "    if f.startswith(\"gpt-4o_semantic_summaries_\") and f.endswith(\"_no.xlsx\") and \"merged\" not in f\n",
    "]\n",
    "\n",
    "# Extract starting index from filenames for sorting\n",
    "def extract_start_index(filename):\n",
    "    match = re.search(r\"gpt-4o_semantic_summaries_(\\d+)_\\d+_no\\.xlsx\", filename)\n",
    "    return int(match.group(1)) if match else float('inf')\n",
    "\n",
    "# Sort files based on starting index\n",
    "summary_files_sorted = sorted(summary_files, key=extract_start_index)\n",
    "\n",
    "# Merge all into one DataFrame\n",
    "df_list = []\n",
    "for file in summary_files_sorted:\n",
    "    path = os.path.join(RESULTS_DIR, file)\n",
    "    try:\n",
    "        df = pd.read_excel(path)\n",
    "        df_list.append(df)\n",
    "        print(f\"‚úÖ Loaded: {file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Skipped {file} due to error: {e}\")\n",
    "\n",
    "# Save merged file\n",
    "if df_list:\n",
    "    merged_df = pd.concat(df_list, ignore_index=True)\n",
    "    merged_df.to_excel(merged_path, index=False)\n",
    "    print(f\"\\n‚úÖ Merged {len(df_list)} files into: {merged_path}\")\n",
    "else:\n",
    "    print(\"‚ùå No matching summary files found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42944b3-7ad3-4086-acba-ee15e0a5018e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x->y prompt for yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a76f23f9-233a-4a5e-947e-c743d4adebd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Processed 50 files...\n",
      "‚úÖ Processed 100 files...\n",
      "‚úÖ Processed 150 files...\n",
      "‚úÖ Processed 200 files...\n",
      "‚úÖ Processed 250 files...\n",
      "‚úÖ Processed 300 files...\n",
      "‚úÖ Processed 350 files...\n",
      "\n",
      "‚úÖ Finished processing range 21-390.\n",
      "üìù Results saved to: M:\\FULL_DATA_COLLECTED\\FINAL_DATASET\\prompt3YesNo\\Yes\\gpt-4o_semantic_summaries_21_390_yes.xlsx\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import openai\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "import chardet\n",
    "import re\n",
    "\n",
    "# === OpenAI client setup ===\n",
    "client = openai.Client(api_key=\"key\")  # Replace with your actual key\n",
    "LLM_NAME = \"gpt-4o\"\n",
    "\n",
    "# === Directories and range config ===\n",
    "PROMPT_DIR = r\"M:\\FULL_DATA_COLLECTED\\FINAL_DATASET\\prompt3YesNo\\Yes\"\n",
    "RESULTS_DIR = PROMPT_DIR\n",
    "\n",
    "RANGE_INPUT = \"21-390\"  # ‚úÖ Update this range as needed\n",
    "\n",
    "# === Extract range ===\n",
    "def extract_range(range_str):\n",
    "    match = re.match(r\"(\\d+)-(\\d+|n)\", range_str)\n",
    "    if match:\n",
    "        start, end = match.groups()\n",
    "        start = int(start)\n",
    "        end = int(end) if end.isdigit() else None\n",
    "        return start, end\n",
    "    return None, None\n",
    "\n",
    "start_idx, end_idx = extract_range(RANGE_INPUT)\n",
    "\n",
    "# === List and filter prompt files ===\n",
    "prompt_files = sorted(\n",
    "    [f for f in os.listdir(PROMPT_DIR) if f.endswith(\".txt\")],\n",
    "    key=lambda x: int(re.search(r'\\d+', x).group()) if re.search(r'\\d+', x) else float('inf')\n",
    ")\n",
    "\n",
    "if start_idx is not None:\n",
    "    prompt_files = prompt_files[start_idx - 1:end_idx]\n",
    "\n",
    "# === Storage for outputs and errors ===\n",
    "results = []\n",
    "error_logs = []\n",
    "\n",
    "# === Process each prompt ===\n",
    "for idx, prompt_file in enumerate(prompt_files, start=start_idx):\n",
    "    prompt_path = os.path.join(PROMPT_DIR, prompt_file)\n",
    "\n",
    "    # Detect encoding\n",
    "    with open(prompt_path, \"rb\") as f:\n",
    "        raw_data = f.read()\n",
    "        encoding = chardet.detect(raw_data)['encoding']\n",
    "\n",
    "    try:\n",
    "        with open(prompt_path, \"r\", encoding=encoding) as f:\n",
    "            prompt_content = f.read().strip()\n",
    "    except Exception as e:\n",
    "        error_logs.append([prompt_file, \"Encoding Error\", str(e)])\n",
    "        continue\n",
    "\n",
    "    # === Send to OpenAI with retries ===\n",
    "    retries = 3\n",
    "    for attempt in range(1, retries + 1):\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=LLM_NAME,\n",
    "                messages=[\n",
    "                    {\"role\": \"user\", \"content\": prompt_content}\n",
    "                ]\n",
    "            )\n",
    "            summary = response.choices[0].message.content.strip()\n",
    "            results.append([prompt_file, summary])\n",
    "            break  # ‚úÖ Success\n",
    "\n",
    "        except openai.RateLimitError:\n",
    "            time.sleep(120)\n",
    "\n",
    "        except openai.APIError as e:\n",
    "            error_logs.append([prompt_file, \"API Error\", str(e)])\n",
    "\n",
    "        except Exception as e:\n",
    "            error_logs.append([prompt_file, \"Unexpected Error\", str(e)])\n",
    "\n",
    "        if attempt == retries:\n",
    "            results.append([prompt_file, \"ERROR: failed after 3 retries\"])\n",
    "\n",
    "    # === Print progress every 50 ===\n",
    "    if idx % 50 == 0:\n",
    "        print(f\"‚úÖ Processed {idx} files...\")\n",
    "\n",
    "    time.sleep(10)  # Adjust if needed to avoid rate limits\n",
    "\n",
    "# === Save summary results ===\n",
    "df_results = pd.DataFrame(results, columns=[\"Prompt File\", \"Semantic Summary\"])\n",
    "filename_out = f\"{LLM_NAME}_semantic_summaries_{RANGE_INPUT.replace('-', '_')}_yes.xlsx\"\n",
    "df_results.to_excel(os.path.join(RESULTS_DIR, filename_out), index=False)\n",
    "\n",
    "# === Save error logs if any ===\n",
    "if error_logs:\n",
    "    df_errors = pd.DataFrame(error_logs, columns=[\"Prompt File\", \"Error Type\", \"Details\"])\n",
    "    filename_err = f\"{LLM_NAME}_errors_{RANGE_INPUT.replace('-', '_')}_yes.xlsx\"\n",
    "    df_errors.to_excel(os.path.join(RESULTS_DIR, filename_err), index=False)\n",
    "\n",
    "# === Final output ===\n",
    "print(f\"\\n‚úÖ Finished processing range {RANGE_INPUT}.\")\n",
    "print(f\"üìù Results saved to: {os.path.join(RESULTS_DIR, filename_out)}\")\n",
    "if error_logs:\n",
    "    print(f\"‚ö†Ô∏è Errors saved to: {os.path.join(RESULTS_DIR, filename_err)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253216aa-4e88-43cd-b0eb-1ebe5f7b065c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5c61c17b-3e4c-403c-bb4f-5cd21c997aa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Merged: gpt-4o_semantic_summaries_1_20_yes.xlsx (20 rows)\n",
      "‚úÖ Merged: gpt-4o_semantic_summaries_21_390_yes.xlsx (370 rows)\n",
      "\n",
      "‚úÖ Final merged YES file saved to:\n",
      "M:\\FULL_DATA_COLLECTED\\FINAL_DATASET\\prompt3YesNo\\Yes\\gpt4o_semantic_summaries_final_yes.xlsx\n",
      "üìä Total rows: 390\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# === Input directory with GPT YES outputs\n",
    "results_dir = r\"M:\\FULL_DATA_COLLECTED\\FINAL_DATASET\\prompt3YesNo\\Yes\"\n",
    "output_file = os.path.join(results_dir, \"gpt4o_semantic_summaries_final_yes.xlsx\")\n",
    "\n",
    "# === Gather all output files\n",
    "files = [\n",
    "    f for f in os.listdir(results_dir)\n",
    "    if f.startswith(\"gpt-4o_semantic_summaries_\") and f.endswith(\"_yes.xlsx\")\n",
    "]\n",
    "\n",
    "merged = []\n",
    "\n",
    "# === Load and tag each file\n",
    "for file in files:\n",
    "    path = os.path.join(results_dir, file)\n",
    "    try:\n",
    "        df = pd.read_excel(path)\n",
    "        df.insert(1, \"Target\", 1)  # ‚úÖ Insert 'Target=1' as second column\n",
    "        merged.append(df)\n",
    "        print(f\"‚úÖ Merged: {file} ({len(df)} rows)\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error reading {file}: {e}\")\n",
    "\n",
    "# === Combine and save\n",
    "if merged:\n",
    "    final = pd.concat(merged, ignore_index=True)\n",
    "    final.to_excel(output_file, index=False)\n",
    "    print(f\"\\n‚úÖ Final merged YES file saved to:\\n{output_file}\")\n",
    "    print(f\"üìä Total rows: {len(final)}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No files found or loaded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26454762-c4cc-42de-a8fe-897112feaff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c0e8e57e-f89f-4716-bbbd-2047afe624be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Processed 50 files...\n",
      "‚úÖ Processed 100 files...\n",
      "‚úÖ Processed 150 files...\n",
      "‚úÖ Processed 200 files...\n",
      "‚úÖ Processed 250 files...\n",
      "‚úÖ Processed 300 files...\n",
      "‚úÖ Processed 350 files...\n",
      "‚úÖ Processed 400 files...\n",
      "\n",
      "‚úÖ Finished processing range 11-432.\n",
      "üìù Results saved to: M:\\FULL_DATA_COLLECTED\\FINAL_DATASET\\prompt3YesNo\\No\\gpt-4o_semantic_summaries_11_432_no.xlsx\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import openai\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "import chardet\n",
    "import re\n",
    "\n",
    "# === OpenAI client setup ===\n",
    "client = openai.Client(api_key=\"key\")  # Replace with your actual key\n",
    "LLM_NAME = \"gpt-4o\"\n",
    "\n",
    "# === Directories and range config ===\n",
    "PROMPT_DIR = r\"M:\\FULL_DATA_COLLECTED\\FINAL_DATASET\\prompt3YesNo\\No\"\n",
    "RESULTS_DIR = PROMPT_DIR\n",
    "\n",
    "RANGE_INPUT = \"11-432\"  # ‚úÖ Update this range as needed\n",
    "\n",
    "# === Extract range ===\n",
    "def extract_range(range_str):\n",
    "    match = re.match(r\"(\\d+)-(\\d+|n)\", range_str)\n",
    "    if match:\n",
    "        start, end = match.groups()\n",
    "        start = int(start)\n",
    "        end = int(end) if end.isdigit() else None\n",
    "        return start, end\n",
    "    return None, None\n",
    "\n",
    "start_idx, end_idx = extract_range(RANGE_INPUT)\n",
    "\n",
    "# === List and filter prompt files ===\n",
    "prompt_files = sorted(\n",
    "    [f for f in os.listdir(PROMPT_DIR) if f.endswith(\".txt\")],\n",
    "    key=lambda x: int(re.search(r'\\d+', x).group()) if re.search(r'\\d+', x) else float('inf')\n",
    ")\n",
    "\n",
    "if start_idx is not None:\n",
    "    prompt_files = prompt_files[start_idx - 1:end_idx]\n",
    "\n",
    "# === Storage for outputs and errors ===\n",
    "results = []\n",
    "error_logs = []\n",
    "\n",
    "# === Process each prompt ===\n",
    "for idx, prompt_file in enumerate(prompt_files, start=start_idx):\n",
    "    prompt_path = os.path.join(PROMPT_DIR, prompt_file)\n",
    "\n",
    "    # Detect encoding\n",
    "    with open(prompt_path, \"rb\") as f:\n",
    "        raw_data = f.read()\n",
    "        encoding = chardet.detect(raw_data)['encoding']\n",
    "\n",
    "    try:\n",
    "        with open(prompt_path, \"r\", encoding=encoding) as f:\n",
    "            prompt_content = f.read().strip()\n",
    "    except Exception as e:\n",
    "        error_logs.append([prompt_file, \"Encoding Error\", str(e)])\n",
    "        continue\n",
    "\n",
    "    # === Send to OpenAI with retries ===\n",
    "    retries = 3\n",
    "    for attempt in range(1, retries + 1):\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=LLM_NAME,\n",
    "                messages=[\n",
    "                    {\"role\": \"user\", \"content\": prompt_content}\n",
    "                ]\n",
    "            )\n",
    "            summary = response.choices[0].message.content.strip()\n",
    "            results.append([prompt_file, summary])\n",
    "            break  # ‚úÖ Success\n",
    "\n",
    "        except openai.RateLimitError:\n",
    "            time.sleep(120)\n",
    "\n",
    "        except openai.APIError as e:\n",
    "            error_logs.append([prompt_file, \"API Error\", str(e)])\n",
    "\n",
    "        except Exception as e:\n",
    "            error_logs.append([prompt_file, \"Unexpected Error\", str(e)])\n",
    "\n",
    "        if attempt == retries:\n",
    "            results.append([prompt_file, \"ERROR: failed after 3 retries\"])\n",
    "\n",
    "    # === Print progress every 50 ===\n",
    "    if idx % 50 == 0:\n",
    "        print(f\"‚úÖ Processed {idx} files...\")\n",
    "\n",
    "    time.sleep(10)  # Adjust if needed to avoid rate limits\n",
    "\n",
    "# === Save summary results ===\n",
    "df_results = pd.DataFrame(results, columns=[\"Prompt File\", \"Semantic Summary\"])\n",
    "filename_out = f\"{LLM_NAME}_semantic_summaries_{RANGE_INPUT.replace('-', '_')}_no.xlsx\"\n",
    "df_results.to_excel(os.path.join(RESULTS_DIR, filename_out), index=False)\n",
    "\n",
    "# === Save error logs if any ===\n",
    "if error_logs:\n",
    "    df_errors = pd.DataFrame(error_logs, columns=[\"Prompt File\", \"Error Type\", \"Details\"])\n",
    "    filename_err = f\"{LLM_NAME}_errors_{RANGE_INPUT.replace('-', '_')}_no.xlsx\"\n",
    "    df_errors.to_excel(os.path.join(RESULTS_DIR, filename_err), index=False)\n",
    "\n",
    "# === Final output ===\n",
    "print(f\"\\n‚úÖ Finished processing range {RANGE_INPUT}.\")\n",
    "print(f\"üìù Results saved to: {os.path.join(RESULTS_DIR, filename_out)}\")\n",
    "if error_logs:\n",
    "    print(f\"‚ö†Ô∏è Errors saved to: {os.path.join(RESULTS_DIR, filename_err)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9083c9-dd3e-4639-96e1-bbfa79d24bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "025ce1ed-1167-414d-ade8-09cb65c78bfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Merged: gpt-4o_semantic_summaries_1_10_no.xlsx (10 rows)\n",
      "‚úÖ Merged: gpt-4o_semantic_summaries_11_432_no.xlsx (422 rows)\n",
      "\n",
      "‚úÖ Final merged NO file saved to:\n",
      "M:\\FULL_DATA_COLLECTED\\FINAL_DATASET\\prompt3YesNo\\No\\gpt4o_semantic_summaries_final_no.xlsx\n",
      "üìä Total rows: 432\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# === Input and output paths\n",
    "results_dir = r\"M:\\FULL_DATA_COLLECTED\\FINAL_DATASET\\prompt3YesNo\\No\"\n",
    "output_file = os.path.join(results_dir, \"gpt4o_semantic_summaries_final_no.xlsx\")\n",
    "\n",
    "# === Collect files\n",
    "files = [\n",
    "    f for f in os.listdir(results_dir)\n",
    "    if f.startswith(\"gpt-4o_semantic_summaries_\") and f.endswith(\"_no.xlsx\")\n",
    "]\n",
    "\n",
    "# === Sort by numeric range start (e.g., from '1_10' or '11_432')\n",
    "def extract_start_index(filename):\n",
    "    match = re.search(r\"_(\\d+)_\\d+_no\\.xlsx\", filename)\n",
    "    return int(match.group(1)) if match else float('inf')\n",
    "\n",
    "files_sorted = sorted(files, key=extract_start_index)\n",
    "\n",
    "# === Load and merge\n",
    "merged = []\n",
    "for file in files_sorted:\n",
    "    path = os.path.join(results_dir, file)\n",
    "    try:\n",
    "        df = pd.read_excel(path)\n",
    "        df.insert(1, \"Target\", 0)  # ‚úÖ Insert as second column\n",
    "        merged.append(df)\n",
    "        print(f\"‚úÖ Merged: {file} ({len(df)} rows)\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error reading {file}: {e}\")\n",
    "\n",
    "# === Save final file\n",
    "if merged:\n",
    "    final = pd.concat(merged, ignore_index=True)\n",
    "    final.to_excel(output_file, index=False)\n",
    "    print(f\"\\n‚úÖ Final merged NO file saved to:\\n{output_file}\")\n",
    "    print(f\"üìä Total rows: {len(final)}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No valid files found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59f3fec-8c62-4eff-8882-0190f6d81c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge yes and no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dd571cdf-66fe-46e1-bb93-257fda4463c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ YES + NO merged in order (YES first).\n",
      "üìÅ Saved to: M:\\FULL_DATA_COLLECTED\\FINAL_DATASET\\prompt3YesNo\\gpt4o_semantic_summaries_final_with_target.xlsx\n",
      "üìä Total rows: 822\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# === Base directory\n",
    "base_dir = r\"M:\\FULL_DATA_COLLECTED\\FINAL_DATASET\\prompt3YesNo\"\n",
    "\n",
    "# === Input files\n",
    "yes_file = os.path.join(base_dir, \"Yes\", \"gpt4o_semantic_summaries_final_yes.xlsx\")\n",
    "no_file  = os.path.join(base_dir, \"No\", \"gpt4o_semantic_summaries_final_no.xlsx\")\n",
    "\n",
    "# === Output file\n",
    "output_file = os.path.join(base_dir, \"gpt4o_semantic_summaries_final_with_target.xlsx\")\n",
    "\n",
    "# === Load both\n",
    "df_yes = pd.read_excel(yes_file)\n",
    "df_no = pd.read_excel(no_file)\n",
    "\n",
    "# === Ensure consistent column order\n",
    "columns = [\"Prompt File\", \"Target\", \"Semantic Summary\"]\n",
    "df_yes = df_yes[columns]\n",
    "df_no = df_no[columns]\n",
    "\n",
    "# === Concatenate YES first, then NO\n",
    "final_df = pd.concat([df_yes, df_no], ignore_index=True)\n",
    "\n",
    "# === Save\n",
    "final_df.to_excel(output_file, index=False)\n",
    "\n",
    "print(f\"\\n‚úÖ YES + NO merged in order (YES first).\")\n",
    "print(f\"üìÅ Saved to: {output_file}\")\n",
    "print(f\"üìä Total rows: {len(final_df)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "132d87d6-5ae1-4b10-b0bf-f6937f13f41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x-> y response\n",
    "# non judgemental answer\n",
    "# yes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b685eca8-b829-4e87-863b-e78d86c33fdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Processed 50 files...\n",
      "‚úÖ Processed 100 files...\n",
      "‚úÖ Processed 150 files...\n",
      "‚úÖ Processed 200 files...\n",
      "‚úÖ Processed 250 files...\n",
      "‚úÖ Processed 300 files...\n",
      "‚úÖ Processed 350 files...\n",
      "\n",
      "‚úÖ Finished processing range 11-390.\n",
      "üìù Results saved to: M:\\FULL_DATA_COLLECTED\\FINAL_DATASET\\prompt4YesNo\\Yes\\gpt-4o_task_action_11_390_yes.xlsx\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import openai\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "import chardet\n",
    "import re\n",
    "\n",
    "# === OpenAI client setup ===\n",
    "client = openai.Client(api_key=\"key\")  # Replace if needed\n",
    "LLM_NAME = \"gpt-4o\"\n",
    "\n",
    "# === Directories and range config ===\n",
    "PROMPT_DIR = r\"M:\\FULL_DATA_COLLECTED\\FINAL_DATASET\\prompt4YesNo\\Yes\"\n",
    "RESULTS_DIR = PROMPT_DIR\n",
    "RANGE_INPUT = \"11-390\"  # ‚úÖ Update as needed\n",
    "\n",
    "# === Extract range ===\n",
    "def extract_range(range_str):\n",
    "    match = re.match(r\"(\\d+)-(\\d+|n)\", range_str)\n",
    "    if match:\n",
    "        start, end = match.groups()\n",
    "        start = int(start)\n",
    "        end = int(end) if end.isdigit() else None\n",
    "        return start, end\n",
    "    return None, None\n",
    "\n",
    "start_idx, end_idx = extract_range(RANGE_INPUT)\n",
    "\n",
    "# === List and filter prompt files ===\n",
    "prompt_files = sorted(\n",
    "    [f for f in os.listdir(PROMPT_DIR) if f.endswith(\".txt\")],\n",
    "    key=lambda x: int(re.search(r'\\d+', x).group()) if re.search(r'\\d+', x) else float('inf')\n",
    ")\n",
    "\n",
    "if start_idx is not None:\n",
    "    prompt_files = prompt_files[start_idx - 1:end_idx]\n",
    "\n",
    "# === Storage for outputs and errors ===\n",
    "results = []\n",
    "error_logs = []\n",
    "\n",
    "# === Process each prompt ===\n",
    "for idx, prompt_file in enumerate(prompt_files, start=start_idx):\n",
    "    prompt_path = os.path.join(PROMPT_DIR, prompt_file)\n",
    "\n",
    "    # Detect encoding\n",
    "    with open(prompt_path, \"rb\") as f:\n",
    "        raw_data = f.read()\n",
    "        encoding = chardet.detect(raw_data)['encoding']\n",
    "\n",
    "    try:\n",
    "        with open(prompt_path, \"r\", encoding=encoding) as f:\n",
    "            prompt_content = f.read().strip()\n",
    "    except Exception as e:\n",
    "        error_logs.append([prompt_file, \"Encoding Error\", str(e)])\n",
    "        continue\n",
    "\n",
    "    # === Send to OpenAI with retries ===\n",
    "    retries = 3\n",
    "    for attempt in range(1, retries + 1):\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=LLM_NAME,\n",
    "                messages=[\n",
    "                    {\"role\": \"user\", \"content\": prompt_content}\n",
    "                ]\n",
    "            )\n",
    "            task_action = response.choices[0].message.content.strip()\n",
    "            results.append([prompt_file, task_action])\n",
    "            break  # ‚úÖ Success\n",
    "\n",
    "        except openai.RateLimitError:\n",
    "            time.sleep(120)\n",
    "\n",
    "        except openai.APIError as e:\n",
    "            error_logs.append([prompt_file, \"API Error\", str(e)])\n",
    "\n",
    "        except Exception as e:\n",
    "            error_logs.append([prompt_file, \"Unexpected Error\", str(e)])\n",
    "\n",
    "        if attempt == retries:\n",
    "            results.append([prompt_file, \"ERROR: failed after 3 retries\"])\n",
    "\n",
    "    # === Print progress every 50 ===\n",
    "    if idx % 50 == 0:\n",
    "        print(f\"‚úÖ Processed {idx} files...\")\n",
    "\n",
    "    time.sleep(10)  # Adjust to control rate\n",
    "\n",
    "# === Save results to Excel\n",
    "df_results = pd.DataFrame(results, columns=[\"Prompt File\", \"Task-Action Response\"])\n",
    "filename_out = f\"{LLM_NAME}_task_action_{RANGE_INPUT.replace('-', '_')}_yes.xlsx\"\n",
    "df_results.to_excel(os.path.join(RESULTS_DIR, filename_out), index=False)\n",
    "\n",
    "# === Save error logs\n",
    "if error_logs:\n",
    "    df_errors = pd.DataFrame(error_logs, columns=[\"Prompt File\", \"Error Type\", \"Details\"])\n",
    "    filename_err = f\"{LLM_NAME}_errors_{RANGE_INPUT.replace('-', '_')}_yes.xlsx\"\n",
    "    df_errors.to_excel(os.path.join(RESULTS_DIR, filename_err), index=False)\n",
    "\n",
    "# === Final output summary\n",
    "print(f\"\\n‚úÖ Finished processing range {RANGE_INPUT}.\")\n",
    "print(f\"üìù Results saved to: {os.path.join(RESULTS_DIR, filename_out)}\")\n",
    "if error_logs:\n",
    "    print(f\"‚ö†Ô∏è Errors saved to: {os.path.join(RESULTS_DIR, filename_err)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20ad195-7d99-4106-8aa0-f5defa3a30d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a6c167f-6b89-4a21-ad35-3dd6fa97ffed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Merged: gpt-4o_task_action_1_10_yes.xlsx (10 rows)\n",
      "‚úÖ Merged: gpt-4o_task_action_11_390_yes.xlsx (380 rows)\n",
      "\n",
      "‚úÖ Final merged YES file saved to:\n",
      "M:\\FULL_DATA_COLLECTED\\FINAL_DATASET\\prompt4YesNo\\Yes\\gpt4o_task_action_final_yes.xlsx\n",
      "üìä Total rows: 390\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# === Input and output paths\n",
    "results_dir = r\"M:\\FULL_DATA_COLLECTED\\FINAL_DATASET\\prompt4YesNo\\Yes\"\n",
    "output_file = os.path.join(results_dir, \"gpt4o_task_action_final_yes.xlsx\")\n",
    "\n",
    "# === Collect files\n",
    "files = [\n",
    "    f for f in os.listdir(results_dir)\n",
    "    if f.startswith(\"gpt-4o_task_action_\") and f.endswith(\"_yes.xlsx\")\n",
    "]\n",
    "\n",
    "# === Sort by numeric range start (e.g., from '1_10' or '11_432')\n",
    "def extract_start_index(filename):\n",
    "    match = re.search(r\"_(\\d+)_\\d+_yes\\.xlsx\", filename)\n",
    "    return int(match.group(1)) if match else float('inf')\n",
    "\n",
    "files_sorted = sorted(files, key=extract_start_index)\n",
    "\n",
    "# === Load and merge\n",
    "merged = []\n",
    "for file in files_sorted:\n",
    "    path = os.path.join(results_dir, file)\n",
    "    try:\n",
    "        df = pd.read_excel(path)\n",
    "        df.insert(1, \"Target\", 1)  # ‚úÖ Insert as second column\n",
    "        merged.append(df)\n",
    "        print(f\"‚úÖ Merged: {file} ({len(df)} rows)\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error reading {file}: {e}\")\n",
    "\n",
    "# === Save final file\n",
    "if merged:\n",
    "    final = pd.concat(merged, ignore_index=True)\n",
    "    final.to_excel(output_file, index=False)\n",
    "    print(f\"\\n‚úÖ Final merged YES file saved to:\\n{output_file}\")\n",
    "    print(f\"üìä Total rows: {len(final)}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No valid files found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b435c8bc-04ca-406c-a113-861806ffa691",
   "metadata": {},
   "outputs": [],
   "source": [
    "# no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3dbae923-2272-4152-b839-7209f5c9af8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Processed 50 files...\n",
      "‚úÖ Processed 100 files...\n",
      "‚úÖ Processed 150 files...\n",
      "‚úÖ Processed 200 files...\n",
      "‚úÖ Processed 250 files...\n",
      "‚úÖ Processed 300 files...\n",
      "‚úÖ Processed 350 files...\n",
      "‚úÖ Processed 400 files...\n",
      "\n",
      "‚úÖ Finished processing range 11-432.\n",
      "üìù Results saved to: M:\\FULL_DATA_COLLECTED\\FINAL_DATASET\\prompt4YesNo\\No\\gpt-4o_task_action_11_432_no.xlsx\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import openai\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "import chardet\n",
    "import re\n",
    "\n",
    "# === OpenAI client setup ===\n",
    "client = openai.Client(api_key=\"key\")  # Replace if needed\n",
    "LLM_NAME = \"gpt-4o\"\n",
    "\n",
    "# === Directories and range config ===\n",
    "PROMPT_DIR = r\"M:\\FULL_DATA_COLLECTED\\FINAL_DATASET\\prompt4YesNo\\No\"\n",
    "RESULTS_DIR = PROMPT_DIR\n",
    "RANGE_INPUT = \"11-432\"  # ‚úÖ Update as needed\n",
    "\n",
    "# === Extract range ===\n",
    "def extract_range(range_str):\n",
    "    match = re.match(r\"(\\d+)-(\\d+|n)\", range_str)\n",
    "    if match:\n",
    "        start, end = match.groups()\n",
    "        start = int(start)\n",
    "        end = int(end) if end.isdigit() else None\n",
    "        return start, end\n",
    "    return None, None\n",
    "\n",
    "start_idx, end_idx = extract_range(RANGE_INPUT)\n",
    "\n",
    "# === List and filter prompt files ===\n",
    "prompt_files = sorted(\n",
    "    [f for f in os.listdir(PROMPT_DIR) if f.endswith(\".txt\")],\n",
    "    key=lambda x: int(re.search(r'\\d+', x).group()) if re.search(r'\\d+', x) else float('inf')\n",
    ")\n",
    "\n",
    "if start_idx is not None:\n",
    "    prompt_files = prompt_files[start_idx - 1:end_idx]\n",
    "\n",
    "# === Storage for outputs and errors ===\n",
    "results = []\n",
    "error_logs = []\n",
    "\n",
    "# === Process each prompt ===\n",
    "for idx, prompt_file in enumerate(prompt_files, start=start_idx):\n",
    "    prompt_path = os.path.join(PROMPT_DIR, prompt_file)\n",
    "\n",
    "    # Detect encoding\n",
    "    with open(prompt_path, \"rb\") as f:\n",
    "        raw_data = f.read()\n",
    "        encoding = chardet.detect(raw_data)['encoding']\n",
    "\n",
    "    try:\n",
    "        with open(prompt_path, \"r\", encoding=encoding) as f:\n",
    "            prompt_content = f.read().strip()\n",
    "    except Exception as e:\n",
    "        error_logs.append([prompt_file, \"Encoding Error\", str(e)])\n",
    "        continue\n",
    "\n",
    "    # === Send to OpenAI with retries ===\n",
    "    retries = 3\n",
    "    for attempt in range(1, retries + 1):\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=LLM_NAME,\n",
    "                messages=[\n",
    "                    {\"role\": \"user\", \"content\": prompt_content}\n",
    "                ]\n",
    "            )\n",
    "            task_action = response.choices[0].message.content.strip()\n",
    "            results.append([prompt_file, task_action])\n",
    "            break  # ‚úÖ Success\n",
    "\n",
    "        except openai.RateLimitError:\n",
    "            time.sleep(120)\n",
    "\n",
    "        except openai.APIError as e:\n",
    "            error_logs.append([prompt_file, \"API Error\", str(e)])\n",
    "\n",
    "        except Exception as e:\n",
    "            error_logs.append([prompt_file, \"Unexpected Error\", str(e)])\n",
    "\n",
    "        if attempt == retries:\n",
    "            results.append([prompt_file, \"ERROR: failed after 3 retries\"])\n",
    "\n",
    "    # === Print progress every 50 ===\n",
    "    if idx % 50 == 0:\n",
    "        print(f\"‚úÖ Processed {idx} files...\")\n",
    "\n",
    "    time.sleep(10)  # Adjust to control rate\n",
    "\n",
    "# === Save results to Excel\n",
    "df_results = pd.DataFrame(results, columns=[\"Prompt File\", \"Task-Action Response\"])\n",
    "filename_out = f\"{LLM_NAME}_task_action_{RANGE_INPUT.replace('-', '_')}_no.xlsx\"\n",
    "df_results.to_excel(os.path.join(RESULTS_DIR, filename_out), index=False)\n",
    "\n",
    "# === Save error logs\n",
    "if error_logs:\n",
    "    df_errors = pd.DataFrame(error_logs, columns=[\"Prompt File\", \"Error Type\", \"Details\"])\n",
    "    filename_err = f\"{LLM_NAME}_errors_{RANGE_INPUT.replace('-', '_')}_no.xlsx\"\n",
    "    df_errors.to_excel(os.path.join(RESULTS_DIR, filename_err), index=False)\n",
    "\n",
    "# === Final output summary\n",
    "print(f\"\\n‚úÖ Finished processing range {RANGE_INPUT}.\")\n",
    "print(f\"üìù Results saved to: {os.path.join(RESULTS_DIR, filename_out)}\")\n",
    "if error_logs:\n",
    "    print(f\"‚ö†Ô∏è Errors saved to: {os.path.join(RESULTS_DIR, filename_err)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16e7303-fdb0-4042-a0db-d0f695052056",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fca40b10-253b-4de5-b04a-2191df51b9b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Merged: gpt-4o_task_action_1_10_no.xlsx (10 rows)\n",
      "‚úÖ Merged: gpt-4o_task_action_11_432_no.xlsx (422 rows)\n",
      "\n",
      "‚úÖ Final merged NO file saved to:\n",
      "M:\\FULL_DATA_COLLECTED\\FINAL_DATASET\\prompt4YesNo\\No\\gpt4o_task_action_final_no.xlsx\n",
      "üìä Total rows: 432\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# === Input and output paths\n",
    "results_dir = r\"M:\\FULL_DATA_COLLECTED\\FINAL_DATASET\\prompt4YesNo\\No\"\n",
    "output_file = os.path.join(results_dir, \"gpt4o_task_action_final_no.xlsx\")\n",
    "\n",
    "# === Collect all matching GPT result files\n",
    "files = [\n",
    "    f for f in os.listdir(results_dir)\n",
    "    if f.startswith(\"gpt-4o_task_action_\") and f.endswith(\"_no.xlsx\")\n",
    "]\n",
    "\n",
    "# === Sort by numeric range start (e.g., from '1_10' or '11_432')\n",
    "def extract_start_index(filename):\n",
    "    match = re.search(r\"_(\\d+)_\\d+_no\\.xlsx\", filename)\n",
    "    return int(match.group(1)) if match else float('inf')\n",
    "\n",
    "files_sorted = sorted(files, key=extract_start_index)\n",
    "\n",
    "# === Load and merge\n",
    "merged = []\n",
    "for file in files_sorted:\n",
    "    path = os.path.join(results_dir, file)\n",
    "    try:\n",
    "        df = pd.read_excel(path)\n",
    "        df.insert(1, \"Target\", 0)  # ‚úÖ Insert Target column = 0 for non-introducing\n",
    "        merged.append(df)\n",
    "        print(f\"‚úÖ Merged: {file} ({len(df)} rows)\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error reading {file}: {e}\")\n",
    "\n",
    "# === Save final merged file\n",
    "if merged:\n",
    "    final = pd.concat(merged, ignore_index=True)\n",
    "    final.to_excel(output_file, index=False)\n",
    "    print(f\"\\n‚úÖ Final merged NO file saved to:\\n{output_file}\")\n",
    "    print(f\"üìä Total rows: {len(final)}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No valid files found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089618e0-1b79-42ac-9c0c-e68702c787ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
